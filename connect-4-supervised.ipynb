{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d06fbb87",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:13.288746Z",
     "iopub.status.busy": "2025-07-09T12:33:13.288421Z",
     "iopub.status.idle": "2025-07-09T12:33:13.301616Z",
     "shell.execute_reply": "2025-07-09T12:33:13.300177Z"
    },
    "papermill": {
     "duration": 0.021113,
     "end_time": "2025-07-09T12:33:13.303727",
     "exception": false,
     "start_time": "2025-07-09T12:33:13.282614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "os.makedirs('checkpoints', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d424ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def argmax_accuracy(output: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the count of correctly predicted samples for a batch based on\n",
    "    matching the output argmax with *any* of the target argmax indices,\n",
    "    returning the count as a tensor suitable for XLA accumulation.\n",
    "\n",
    "    Args:\n",
    "        output (torch.Tensor): Model output tensor of shape (n, w), where n is\n",
    "                               batch size and w is the vector dimension.\n",
    "        target (torch.Tensor): Target tensor of the same shape (n, w).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A scalar tensor containing the count of correct predictions\n",
    "                      for the batch, located on the same device as inputs.\n",
    "                      Returns a zero tensor if batch size is 0.\n",
    "    \"\"\"\n",
    "    if output.shape != target.shape:\n",
    "        raise ValueError(f\"Output and target tensors must have the same shape. Got {output.shape} and {target.shape}\")\n",
    "    if output.dim() != 2:\n",
    "         raise ValueError(f\"Output and target tensors must be 2D (n, w). Got {output.dim()} dimensions.\")\n",
    "\n",
    "    n, w = output.shape\n",
    "\n",
    "    if n == 0:\n",
    "        # Return a scalar tensor on the correct device\n",
    "        return torch.tensor(0, dtype=torch.long, device=output.device) # Use long for counts\n",
    "\n",
    "    # 0. Find the index of the maximum value in the output (common for both calculations)\n",
    "    output_argmax = torch.argmax(output, dim=1)\n",
    "    \n",
    "    # --- Calculation for the original target ---\n",
    "\n",
    "    # 1. Find the maximum value in the original target\n",
    "    target_max_values = torch.max(target, dim=1, keepdim=True)[0]\n",
    "\n",
    "    # 2. Create a boolean mask for the original target\n",
    "    target_max_mask = (target == target_max_values)\n",
    "\n",
    "    # 3. Check if the output_argmax index is True in the original target_max_mask\n",
    "    # Ensure arange is on the same device as the tensors\n",
    "    correct_predictions_bool_original = target_max_mask[torch.arange(n, device=output.device), output_argmax]\n",
    "\n",
    "    # 4. Sum the boolean tensor to get the count of correct predictions for original target.\n",
    "    correct_count_original = correct_predictions_bool_original.sum()\n",
    "\n",
    "    # --- Calculation for the clipped target ---\n",
    "\n",
    "    # 1. Clip the target values to (-1, 1)\n",
    "    target_clipped = torch.clip(target, -1, 1)\n",
    "\n",
    "    # 2. Find the maximum value in the clipped target\n",
    "    target_clipped_max_values = torch.max(target_clipped, dim=1, keepdim=True)[0]\n",
    "\n",
    "    # 3. Create a boolean mask for the clipped target\n",
    "    target_clipped_max_mask = (target_clipped == target_clipped_max_values)\n",
    "\n",
    "    # 4. Check if the output_argmax index is True in the clipped target_max_mask\n",
    "    correct_predictions_bool_clipped = target_clipped_max_mask[torch.arange(n, device=output.device), output_argmax]\n",
    "\n",
    "    # 5. Sum the boolean tensor to get the count of correct predictions for clipped target.\n",
    "    correct_count_clipped = correct_predictions_bool_clipped.sum()\n",
    "\n",
    "    return correct_count_original, correct_count_clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f791ba75",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:13.313332Z",
     "iopub.status.busy": "2025-07-09T12:33:13.312959Z",
     "iopub.status.idle": "2025-07-09T12:33:13.325850Z",
     "shell.execute_reply": "2025-07-09T12:33:13.324447Z"
    },
    "papermill": {
     "duration": 0.019772,
     "end_time": "2025-07-09T12:33:13.327606",
     "exception": false,
     "start_time": "2025-07-09T12:33:13.307834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNorm2d(nn.LayerNorm):\n",
    "    \"\"\" LayerNorm for channels of '2D' spatial NCHW tensors \"\"\"\n",
    "    def __init__(self, num_channels, eps=1e-6, affine=True):\n",
    "        super().__init__(num_channels, eps=eps, elementwise_affine=affine)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        reduced_channel = max(channel // reduction, 1)\n",
    "        self.fc_scale = nn.Sequential(\n",
    "            nn.Linear(channel, reduced_channel, bias=True),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Linear(reduced_channel, channel, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.fc_offset = nn.Sequential(\n",
    "            nn.Linear(channel, reduced_channel, bias=True),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Linear(reduced_channel, channel, bias=True),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y_scale = self.fc_scale(y).view(b, c, 1, 1)\n",
    "        y_offset = self.fc_offset(y).view(b, c, 1, 1)\n",
    "        return x * y_scale.expand_as(x) + y_offset.expand_as(x)\n",
    "\n",
    "class InitialExtractor(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super(InitialExtractor, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.BatchNorm2d(in_channels))\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2, stride=1, bias=False))\n",
    "        self.convs = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convs(x)\n",
    "        return out\n",
    "\n",
    "class Grouped1x1SumConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_groups):\n",
    "        super().__init__()\n",
    "        assert in_channels % num_groups == 0, \"in_channels must be divisible by num_groups\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_groups = num_groups\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels * num_groups,\n",
    "            kernel_size=1,\n",
    "            groups=num_groups,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, H, W = x.shape\n",
    "        G = self.num_groups\n",
    "        Cout = self.out_channels\n",
    "\n",
    "        out = self.conv(x)\n",
    "\n",
    "        out = out.view(B, G, Cout, H, W)\n",
    "        out = out.sum(dim=1)\n",
    "        return out\n",
    "\n",
    "class ConnectFourBlock(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size = (5, 5), mlp_factor=4, group_factor=2):\n",
    "        super().__init__()\n",
    "\n",
    "        ks_h, ks_w = kernel_size\n",
    "        hidden_dim = in_channels * mlp_factor\n",
    "        layers_0 = []\n",
    "\n",
    "        layers_0.append(nn.Conv2d(in_channels, in_channels, kernel_size=(ks_h, ks_w),  padding=(ks_h // 2, ks_w // 2), groups=in_channels, bias=False))\n",
    "        layers_0.append(LayerNorm2d(in_channels))\n",
    "\n",
    "        layers_0.append(nn.Conv2d(in_channels, hidden_dim, kernel_size=1, bias=False))\n",
    "        layers_0.append(nn.SiLU(inplace=True))\n",
    "\n",
    "        layers_0.append(nn.Conv2d(hidden_dim, in_channels, kernel_size=1, bias=False))\n",
    "\n",
    "        self.conv_next = nn.Sequential(*layers_0)\n",
    "\n",
    "        hidden_dim = in_channels * group_factor\n",
    "        n_groups = 2 * group_factor\n",
    "        ks_h, ks_w = 3, 3\n",
    "        \n",
    "        layers_1 = []\n",
    "        layers_1.append(LayerNorm2d(in_channels))\n",
    "        layers_1.append(nn.Conv2d(in_channels, hidden_dim, kernel_size=1, bias=False))\n",
    "        self.proj_up = nn.Sequential(*layers_1)\n",
    "\n",
    "        layers_2 = []\n",
    "        layers_2.append(nn.Conv2d(hidden_dim, hidden_dim, kernel_size=(ks_h, ks_w), padding=(ks_h // 2, ks_w // 2), groups=n_groups, bias=False))\n",
    "        layers_2.append(Grouped1x1SumConv(hidden_dim, hidden_dim, num_groups=n_groups))\n",
    "        layers_2.append(nn.SiLU(inplace=True))\n",
    "        self.grouped_0 = nn.Sequential(*layers_2)\n",
    "\n",
    "        layers_3 = []\n",
    "        layers_3.append(nn.Conv2d(hidden_dim, hidden_dim, kernel_size=(ks_h, ks_w), padding=(ks_h // 2, ks_w // 2), groups=n_groups, bias=False))\n",
    "        layers_3.append(Grouped1x1SumConv(hidden_dim, hidden_dim, num_groups=n_groups))\n",
    "        layers_3.append(nn.SiLU(inplace=True))\n",
    "        self.grouped_1 = nn.Sequential(*layers_3)\n",
    "\n",
    "        layers_4 = []\n",
    "        layers_4.append(SEBlock(hidden_dim))\n",
    "        layers_4.append(nn.Conv2d(hidden_dim, in_channels, kernel_size=1, bias=False))\n",
    "        self.proj_down = nn.Sequential(*layers_4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        summand = self.conv_next(x)\n",
    "        out = out + summand\n",
    "\n",
    "        up_proj = self.proj_up(out)\n",
    "        summand = self.grouped_0(up_proj)\n",
    "        up_proj = up_proj + summand\n",
    "        summand = self.grouped_1(up_proj)\n",
    "        up_proj = up_proj + summand\n",
    "\n",
    "        summand = self.proj_down(up_proj)\n",
    "        out = out + summand\n",
    "        \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a4b8f7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:13.336788Z",
     "iopub.status.busy": "2025-07-09T12:33:13.336481Z",
     "iopub.status.idle": "2025-07-09T12:33:13.344303Z",
     "shell.execute_reply": "2025-07-09T12:33:13.342802Z"
    },
    "papermill": {
     "duration": 0.01485,
     "end_time": "2025-07-09T12:33:13.346463",
     "exception": false,
     "start_time": "2025-07-09T12:33:13.331613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    return nn.init.trunc_normal_(tensor, mean=mean, std=std, a=a, b=b)\n",
    "\n",
    "class VisionTransformerWithTasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape,\n",
    "        num_layers,\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        mlp_dim,\n",
    "        num_tasks=7 + 7 + 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        C, H, W = input_shape\n",
    "        self.num_patches = H * W\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.proj = nn.Linear(C, embed_dim)\n",
    "        self.task_tokens = nn.Parameter(torch.randn(1, num_tasks, embed_dim))\n",
    "        self.pos_embed = nn.Embedding(self.num_patches + num_tasks, embed_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=mlp_dim,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\"\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.output_head = nn.Conv1d(\n",
    "            in_channels=embed_dim * num_tasks,\n",
    "            out_channels=num_tasks,\n",
    "            kernel_size=1,\n",
    "            groups=num_tasks\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        trunc_normal_(self.task_tokens, std=0.02)\n",
    "        trunc_normal_(self.pos_embed.weight, std=0.02)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Conv1d):\n",
    "                trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        task_tokens = self.task_tokens.expand(B, -1, -1)\n",
    "        x = torch.cat((task_tokens, x), dim=1)\n",
    "\n",
    "        pos_ids = torch.arange(x.size(1), device=x.device).unsqueeze(0).expand(B, -1)\n",
    "        x = x + self.pos_embed(pos_ids)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "        task_outputs = x[:, :self.num_tasks]\n",
    "\n",
    "        task_outputs = task_outputs.reshape(B, self.num_tasks * self.embed_dim, 1)\n",
    "        logits = self.output_head(task_outputs).squeeze(-1)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d179f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyHead(nn.Module):\n",
    "    def __init__(self, in_channels, input_height, input_width, expansion_factor_c=4, expansion_factor_w=4):\n",
    "        super(PolicyHead, self).__init__()\n",
    "        intermediate_channels = expansion_factor_c * in_channels\n",
    "        self.conv_reduce_height_pw_0 = nn.Conv2d(in_channels, intermediate_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.conv_reduce_height_pw_1 = nn.Conv2d(intermediate_channels, intermediate_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.conv_reduce_height_dw = nn.Conv2d(intermediate_channels, intermediate_channels, kernel_size=(input_height, 1), stride=(input_height, 1), groups=intermediate_channels, bias=False)\n",
    "        self.conv_reduce_height_bn = nn.BatchNorm2d(intermediate_channels)\n",
    "        self.silu_reduce_height_1 = nn.SiLU(inplace=True)\n",
    "        self.silu_reduce_height_2 = nn.SiLU(inplace=True)\n",
    "        self.conv_expand_pw = nn.Conv2d(intermediate_channels, in_channels, kernel_size=1, stride=1, bias=False)\n",
    "\n",
    "        self.expansion_factor_w = expansion_factor_w\n",
    "        intermediate_channels = expansion_factor_w * input_width\n",
    "        self.conv_permuted_input_width_as_channels = nn.Conv2d(\n",
    "            in_channels=input_width, \n",
    "            out_channels=intermediate_channels, \n",
    "            kernel_size=1, \n",
    "            stride=1, \n",
    "            bias=False\n",
    "        )\n",
    "        self.silu_permuted_input_width_as_channels = nn.SiLU(inplace=True)\n",
    "\n",
    "        intermediate_channels = expansion_factor_w * in_channels\n",
    "        self.final_logits_conv = nn.Conv2d(\n",
    "            in_channels=intermediate_channels, \n",
    "            out_channels=1,\n",
    "            kernel_size=1, \n",
    "            stride=1, \n",
    "            bias=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv_reduce_height_pw_0(x)\n",
    "        out = self.conv_reduce_height_pw_1(out)\n",
    "        out = self.silu_reduce_height_1(out)\n",
    "        out = self.conv_reduce_height_dw(out)\n",
    "        out = self.conv_reduce_height_bn(out)\n",
    "        out = self.silu_reduce_height_2(out)\n",
    "        out = self.conv_expand_pw(out)\n",
    "        \n",
    "        B, C2, H_one, W_orig = out.shape \n",
    "        \n",
    "        permuted_for_conv = out.permute(0, 3, 2, 1).contiguous()\n",
    "        \n",
    "         # Shape: (B, W_orig, 1, C2)\n",
    "        convolved_output = self.conv_permuted_input_width_as_channels(permuted_for_conv)\n",
    "        convolved_output = self.silu_permuted_input_width_as_channels(convolved_output)\n",
    "        \n",
    "        # Shape: (B, k_expansion_factor * W_orig, 1, C2)\n",
    "        current_tensor = convolved_output.permute(0, 3, 2, 1).contiguous()\n",
    "        # Shape: (B, C2, 1, self.k_expansion_factor * W_orig)\n",
    "        \n",
    "        # --- Step 2: Rearrange to (B, k*C2, 1, W_orig) ---\n",
    "        k = self.expansion_factor_w\n",
    "        \n",
    "        #(B, C2, 1, k * W_orig)\n",
    "        temp_rearrange = current_tensor.squeeze(2) \n",
    "        #(B, C2, k * W_orig)\n",
    "        \n",
    "        temp_rearrange = temp_rearrange.view(B, C2, k, W_orig)\n",
    "        #(B, C2, k, W_orig)\n",
    "        \n",
    "        temp_rearrange = temp_rearrange.permute(0, 2, 1, 3) \n",
    "        #(B, k, C2, W_orig)\n",
    "        \n",
    "        temp_rearrange = temp_rearrange.contiguous().view(B, k * C2, W_orig)\n",
    "        #(B, k*C2, W_orig)\n",
    "        \n",
    "        tensor_for_final_conv = temp_rearrange.unsqueeze(2)\n",
    "        #(B, k*C2, 1, W_orig)\n",
    "\n",
    "        logits_intermediate = self.final_logits_conv(tensor_for_final_conv)\n",
    "        # (B, 1, 1, W_orig)\n",
    "\n",
    "        policy_logits = logits_intermediate.view(B, W_orig)\n",
    "        # Shape: (B, W_orig)\n",
    "\n",
    "        return policy_logits\n",
    "\n",
    "class ValueHead(nn.Module):\n",
    "    def __init__(self, in_channels, input_height, input_width, expansion_factor_c=4, expansion_factor_w=2):\n",
    "        super(ValueHead, self).__init__()\n",
    "        intermediate_channels = expansion_factor_c * in_channels\n",
    "        self.conv_reduce_height_pw_0 = nn.Conv2d(in_channels, intermediate_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.conv_reduce_height_pw_1 = nn.Conv2d(intermediate_channels, intermediate_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.conv_reduce_height_dw = nn.Conv2d(intermediate_channels, intermediate_channels, kernel_size=(input_height, 1), stride=(input_height, 1), groups=intermediate_channels, bias=False)\n",
    "        self.conv_reduce_height_bn = nn.BatchNorm2d(intermediate_channels)\n",
    "        self.silu_reduce_height_1 = nn.SiLU(inplace=True)\n",
    "        self.silu_reduce_height_2 = nn.SiLU(inplace=True)\n",
    "        self.conv_expand_pw = nn.Conv2d(intermediate_channels, in_channels, kernel_size=1, stride=1, bias=False)\n",
    "\n",
    "        self.expansion_factor_w = expansion_factor_w\n",
    "        intermediate_channels = expansion_factor_w * input_width\n",
    "        self.conv_permuted_input_width_as_channels = nn.Conv2d(\n",
    "            in_channels=input_width, \n",
    "            out_channels=intermediate_channels, \n",
    "            kernel_size=1, \n",
    "            stride=1, \n",
    "            bias=False\n",
    "        )\n",
    "        self.silu_flattened_output = nn.SiLU(inplace=True)\n",
    "        \n",
    "        final_flattened_features = intermediate_channels * 1 * in_channels\n",
    "        self.final_linear = nn.Linear(final_flattened_features, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv_reduce_height_pw_0(x)\n",
    "        out = self.conv_reduce_height_pw_1(out)\n",
    "        out = self.silu_reduce_height_1(out)\n",
    "        out = self.conv_reduce_height_dw(out)\n",
    "        out = self.conv_reduce_height_bn(out)\n",
    "        out = self.silu_reduce_height_2(out)\n",
    "        out = self.conv_expand_pw(out)\n",
    "        \n",
    "        B, C2, H_one, W_orig = out.shape \n",
    "        \n",
    "        permuted_for_conv = out.permute(0, 3, 2, 1).contiguous()\n",
    "        \n",
    "        # Shape: (B, W_orig, 1, C2)\n",
    "        convolved_output = self.conv_permuted_input_width_as_channels(permuted_for_conv)\n",
    "        flattened_output = convolved_output.view(B, -1) \n",
    "        silu_output = self.silu_flattened_output(flattened_output)\n",
    "        scalar_value = self.final_linear(silu_output)\n",
    "        \n",
    "        return scalar_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "195c59c3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:13.356077Z",
     "iopub.status.busy": "2025-07-09T12:33:13.355730Z",
     "iopub.status.idle": "2025-07-09T12:33:13.364522Z",
     "shell.execute_reply": "2025-07-09T12:33:13.363124Z"
    },
    "papermill": {
     "duration": 0.015723,
     "end_time": "2025-07-09T12:33:13.366351",
     "exception": false,
     "start_time": "2025-07-09T12:33:13.350628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_channels = 2\n",
    "input_height = 6\n",
    "input_width = 7\n",
    "output_width = 7\n",
    "\n",
    "class Connect4ModelVit(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        extractor_layers = []\n",
    "        stage_0_width = 64\n",
    "        num_conv_blocks = 5\n",
    "        num_transformer_layers = 5\n",
    "        \n",
    "        transformer_args = {\n",
    "            \"input_shape\" : (stage_0_width, input_height, input_width),\n",
    "            \"num_layers\" : num_transformer_layers,\n",
    "            \"embed_dim\" : 256,\n",
    "            \"num_heads\" : 4,\n",
    "            \"mlp_dim\" : 1024,\n",
    "        }\n",
    "        \n",
    "        extractor_layers.append(\n",
    "            InitialExtractor(in_channels=input_channels, out_channels=stage_0_width),\n",
    "        )\n",
    "        \n",
    "        extractor_layers.extend([\n",
    "            ConnectFourBlock(in_channels=stage_0_width,) for _ in range(num_conv_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.conv_extractor = nn.Sequential(*extractor_layers)\n",
    "\n",
    "        self.transformer = VisionTransformerWithTasks(\n",
    "            **transformer_args\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        convnet_features = self.conv_extractor(x)\n",
    "        concatenated_output = self.transformer(convnet_features)\n",
    "\n",
    "        policy_outputs, value_outputs, next_value_outputs \\\n",
    "            = torch.split(concatenated_output, [7, 1, 7], dim=-1)\n",
    "        return policy_outputs, value_outputs, next_value_outputs\n",
    "    \n",
    "class Connect4ModelCvn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        extractor_layers = []\n",
    "        stage_0_width = 64\n",
    "        num_conv_blocks = 7\n",
    "        \n",
    "        extractor_layers.append(\n",
    "            InitialExtractor(in_channels=input_channels, out_channels=stage_0_width),\n",
    "        )\n",
    "        \n",
    "        extractor_layers.extend([\n",
    "            ConnectFourBlock(in_channels=stage_0_width,) for _ in range(num_conv_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.conv_extractor = nn.Sequential(*extractor_layers)\n",
    "\n",
    "        self.policy_logits_head = nn.Sequential(\n",
    "                ConnectFourBlock(in_channels=stage_0_width,),\n",
    "                PolicyHead(\n",
    "                    in_channels=64, \n",
    "                    input_height=input_height,\n",
    "                    input_width=input_width)\n",
    "            )\n",
    "\n",
    "        self.policy_regression_head = nn.Sequential(\n",
    "                ConnectFourBlock(in_channels=stage_0_width,),\n",
    "                PolicyHead(\n",
    "                    in_channels=64, \n",
    "                    input_height=input_height,\n",
    "                    input_width=input_width)\n",
    "            )\n",
    "\n",
    "        self.value_head = nn.Sequential(\n",
    "                ConnectFourBlock(in_channels=stage_0_width,),\n",
    "                ValueHead(\n",
    "                    in_channels=64, \n",
    "                    input_height=input_height,\n",
    "                    input_width=input_width)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        convnet_features = self.conv_extractor(x)\n",
    "\n",
    "        policy_outputs = self.policy_logits_head(convnet_features)\n",
    "        value_outputs = self.value_head(convnet_features)\n",
    "        next_value_outputs = self.policy_regression_head(convnet_features)\n",
    "        return policy_outputs, value_outputs, next_value_outputs\n",
    "    \n",
    "def get_custom_model(model_type=\"vit_medium\"):\n",
    "    if model_type == \"vit_medium\":\n",
    "        return Connect4ModelVit()\n",
    "    if model_type == \"cvn_tiny\":\n",
    "        return Connect4ModelCvn()\n",
    "    raise Exception(f\"unknown model_type: {model_type}\")\n",
    "\n",
    "def count_model_flops(model=None):\n",
    "    from torch.utils.flop_counter import FlopCounterMode\n",
    "    if model is None:\n",
    "        model = get_custom_model()\n",
    "    print(\"Model Architecture:\")\n",
    "    print(model)\n",
    "    \n",
    "    dummy_input = torch.randn(1, input_channels, input_height, input_width)\n",
    "    \n",
    "    with FlopCounterMode(model) as count:\n",
    "        dummy_output = model(dummy_input)\n",
    "        total_flops = count.get_total_flops()\n",
    "    \n",
    "    print(f\"Total FLOPS: {total_flops}\")\n",
    "    print(\"Dummy Input Shape:\", dummy_input.shape)\n",
    "    print(\"Dummy Output:\", dummy_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f358edb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:13.375795Z",
     "iopub.status.busy": "2025-07-09T12:33:13.375489Z",
     "iopub.status.idle": "2025-07-09T12:33:13.383441Z",
     "shell.execute_reply": "2025-07-09T12:33:13.382531Z"
    },
    "papermill": {
     "duration": 0.014449,
     "end_time": "2025-07-09T12:33:13.384827",
     "exception": false,
     "start_time": "2025-07-09T12:33:13.370378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def move_to_device(batch, device):\n",
    "        obs, targets = batch\n",
    "        obs = obs.to(device, torch.float32)\n",
    "        targets = targets.to(device, torch.float32)\n",
    "        return obs, targets\n",
    "\n",
    "mse_loss = nn.MSELoss(reduction='none')\n",
    "kl_criterion = nn.KLDivLoss(reduction='none', log_target=True)\n",
    "\n",
    "def calculate_loss(policy_outputs, value_outputs, next_value_outputs, labels):\n",
    "    target_policy_log_probs = F.log_softmax(labels / 10, dim=-1)\n",
    "    policy_log_probs = F.log_softmax(policy_outputs / 4, dim=-1)\n",
    "\n",
    "    _, best_choice_indices = torch.max(torch.clamp(labels, min=-1, max=1), dim=-1)\n",
    "    bad_choice_mask = torch.ones_like(policy_log_probs, dtype=torch.float)\n",
    "    bad_choice_mask.scatter_(1, best_choice_indices.unsqueeze(1), 0)\n",
    "    bad_choice_loss = - (policy_log_probs * bad_choice_mask).mean(dim=-1)\n",
    "\n",
    "    policy_loss = kl_criterion(policy_log_probs, target_policy_log_probs).sum(dim=1)\n",
    "    policy_loss = 0.8 * policy_loss + 0.002 * bad_choice_loss\n",
    "    \n",
    "    value_loss = mse_loss(value_outputs, torch.max(labels, dim=-1).values.unsqueeze(1)).mean(dim=1)\n",
    "    next_value_loss = mse_loss(next_value_outputs, labels).mean(dim=1)\n",
    "\n",
    "    combined_loss = 10000 * policy_loss + value_loss + 0.1 * next_value_loss\n",
    "\n",
    "    loss_parts = {\n",
    "        'combined': combined_loss,\n",
    "        'policy': policy_loss,\n",
    "        'value': value_loss,\n",
    "        'next_value': next_value_loss\n",
    "    }\n",
    "\n",
    "    return combined_loss, loss_parts\n",
    "\n",
    "def validate_model(model, val_loader, device, max_batches=None, use_tqdm=False, mine_hard=False):\n",
    "    model.to(device)\n",
    "    \n",
    "    if mine_hard:\n",
    "        hard_examples = []\n",
    "        \n",
    "    ema_loss = 0.0\n",
    "    ema_alpha = 0.05\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    num_samples_in_val = 0\n",
    "    val_loss = kl_loss_total = value_loss_total = next_value_loss_total = 0.0\n",
    "    val_correct_count_strong = val_correct_count_weak = 0.0\n",
    "    val_correct_count_strong_nv = val_correct_count_weak_nv = 0.0\n",
    "    loader_slice =  itertools.islice(val_loader, max_batches) if max_batches else val_loader\n",
    "    if use_tqdm:\n",
    "        loader_slice = tqdm(loader_slice)\n",
    "    with torch.no_grad():\n",
    "        for batch in loader_slice:\n",
    "            inputs, labels = move_to_device(batch, device)\n",
    "            policy_outputs, value_outputs, next_value_outputs = model(inputs)\n",
    "\n",
    "            loss, loss_parts = calculate_loss(policy_outputs, value_outputs, next_value_outputs, labels)\n",
    "            \n",
    "            batch_correct_count_strong, batch_correct_count_weak = argmax_accuracy(policy_outputs, labels)\n",
    "            val_correct_count_strong += batch_correct_count_strong\n",
    "            val_correct_count_weak += batch_correct_count_weak\n",
    "            \n",
    "            batch_correct_count_strong_nv, batch_correct_count_weak_nv = argmax_accuracy(next_value_outputs, labels)\n",
    "            val_correct_count_strong_nv += batch_correct_count_strong_nv\n",
    "            val_correct_count_weak_nv += batch_correct_count_weak_nv\n",
    "            \n",
    "            num_samples_in_batch = inputs.size(0)\n",
    "            batch_loss_mean = loss.mean(dim=0).item()\n",
    "            val_loss += batch_loss_mean * num_samples_in_batch\n",
    "            kl_loss_total += loss_parts[\"policy\"].sum(dim=0).item()\n",
    "            value_loss_total += loss_parts[\"value\"].sum(dim=0).item()\n",
    "            next_value_loss_total += loss_parts[\"next_value\"].sum(dim=0).item()\n",
    "            num_samples_in_val += num_samples_in_batch\n",
    "            \n",
    "            if mine_hard:\n",
    "                # Update the exponential moving average (EMA) of the loss\n",
    "                if ema_loss == 0.0:\n",
    "                    ema_loss = 0.8 * batch_loss_mean\n",
    "                else:\n",
    "                    ema_loss = ema_alpha * batch_loss_mean + (1 - ema_alpha) * ema_loss\n",
    "                \n",
    "                threshold = ema_loss * 10\n",
    "                hard_example_indices = torch.where(loss > threshold)[0]\n",
    "                \n",
    "                if len(hard_example_indices) > 0:\n",
    "                    hard_inputs = inputs[hard_example_indices]\n",
    "                    hard_labels = labels[hard_example_indices]\n",
    "                    hard_losses = loss[hard_example_indices]\n",
    "                    hard_examples.append((hard_inputs, hard_labels, hard_losses))\n",
    "            \n",
    "    epoch_val_loss = {\n",
    "            \"combined\": val_loss / num_samples_in_val, \"policy\": kl_loss_total / num_samples_in_val,\n",
    "            \"value\": value_loss_total / num_samples_in_val, \"next_value\": next_value_loss_total / num_samples_in_val,\n",
    "        }\n",
    "    epoch_val_policy_acc = {\n",
    "            \"strong\": val_correct_count_strong / num_samples_in_val, \"weak\": val_correct_count_weak / num_samples_in_val,\n",
    "            \"strong_nv\": val_correct_count_strong_nv / num_samples_in_val, \"weak_nv\": val_correct_count_weak_nv / num_samples_in_val,\n",
    "        }\n",
    "\n",
    "    print(f\"Validation Loss: {epoch_val_loss}\")\n",
    "    print(f\"Validation Policy Acc: {epoch_val_policy_acc}\")\n",
    "    if mine_hard:\n",
    "        if len(hard_examples) > 0:\n",
    "            hard_inputs_tensor = torch.cat([ex[0] for ex in hard_examples], dim=0)\n",
    "            hard_labels_tensor = torch.cat([ex[1] for ex in hard_examples], dim=0)\n",
    "            hard_losses_tensor = torch.cat([ex[2] for ex in hard_examples], dim=0)\n",
    "            return hard_inputs_tensor, hard_labels_tensor, hard_losses_tensor\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    return epoch_val_policy_acc\n",
    "\n",
    "def train_model(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    model: nn.Module,\n",
    "    device,\n",
    "    checkpoint_path: str = \"checkpoint.pth\",\n",
    "    best_model_path: str = \"best_model.pth\",\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 0.001,\n",
    "    epochs: int = 10,\n",
    "    num_batches_per_epoch = 10,\n",
    "    warmup_fraction: float = 0.1,\n",
    "    weight_decay: float = 0.01,\n",
    "    use_tqdm=False,\n",
    "):  \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    total_steps = num_batches_per_epoch * epochs\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, math.ceil(total_steps * warmup_fraction), total_steps)\n",
    "        \n",
    "    start_epoch = 0\n",
    "    best_val_acc = 0\n",
    "    model.to(device)\n",
    "    \n",
    "    epoch = start_epoch - 1\n",
    "    step = -1\n",
    "    model.train()\n",
    "    continue_training = True\n",
    "    \n",
    "    # Training loop\n",
    "    while continue_training:\n",
    "        if use_tqdm:\n",
    "            loader_subset = tqdm(train_loader)\n",
    "        for batch in loader_subset:\n",
    "            step += 1\n",
    "            if step >= num_batches_per_epoch:\n",
    "                step = 0\n",
    "                running_loss_avg = {loss_name: loss_value / num_samples_in_epoch for loss_name, loss_value in running_loss.items()}\n",
    "                print(f\"Epoch [{epoch}/{epochs}] done, Training Loss: {running_loss_avg}\")\n",
    "                \n",
    "                epoch_val_policy_acc = validate_model(model, val_loader, device, num_batches_per_epoch, use_tqdm=use_tqdm)\n",
    "                # Save checkpoint and best model based on validation accuracy\n",
    "                if epoch_val_policy_acc[\"strong\"] > best_val_acc:\n",
    "                    best_val_acc = epoch_val_policy_acc[\"strong\"]\n",
    "                    torch.save(model.state_dict(), best_model_path)\n",
    "                    print(f\"Validation accuracy improved. Saved best model to {best_model_path}\")\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                \n",
    "            if step == 0:\n",
    "                epoch += 1\n",
    "                running_loss = defaultdict(float)\n",
    "                num_samples_in_epoch = 0\n",
    "                if epoch >= epochs:\n",
    "                    continue_training = False\n",
    "                    break\n",
    "                print(f\"Begin epoch {epoch} with LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "                \n",
    "            inputs, labels = move_to_device(batch, device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            policy_outputs, value_outputs, next_value_outputs = model(inputs)\n",
    "\n",
    "            combined_loss, loss_parts = calculate_loss(policy_outputs, value_outputs, next_value_outputs, labels)\n",
    "\n",
    "            combined_loss.mean(dim=0).backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            num_samples_in_epoch += inputs.size(0)\n",
    "            for loss_name, loss_value in loss_parts.items():\n",
    "                running_loss[loss_name] += loss_value.sum(dim=0).item()\n",
    "            if (step + 1) % math.ceil(num_batches_per_epoch / 10) == 0:\n",
    "                running_loss_avg = {loss_name: loss_value / num_samples_in_epoch for loss_name, loss_value in running_loss.items()}\n",
    "                print(f\"Epoch [{epoch}/{epochs}], Step [{step} / {num_batches_per_epoch}], Running Loss: {running_loss_avg}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    print(f\"Best validation acc achieved: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2688eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def load_dataset(dataset_path):\n",
    "    import numpy as np\n",
    "    import gc\n",
    "    import torch\n",
    "\n",
    "    try:\n",
    "        print(\"Loading dataset with mmap_mode='r'...\")\n",
    "\n",
    "        dataset = np.load(dataset_path, mmap_mode='r')\n",
    "        train_obs_np = dataset[\"x_train\"]\n",
    "        train_targets_np = dataset[\"y_train\"]\n",
    "        val_obs_np = dataset[\"x_val\"]\n",
    "        val_targets_np = dataset[\"y_val\"]\n",
    "\n",
    "        del dataset\n",
    "        gc.collect() # Force garbage collection\n",
    "\n",
    "        print(\"Converting NumPy arrays to PyTorch tensors with float16 precision (1/4)...\")\n",
    "        train_obs = torch.from_numpy(train_obs_np).to(dtype=torch.float16)\n",
    "        del train_obs_np\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Converting NumPy arrays to PyTorch tensors with float16 precision (2/4)...\")\n",
    "        val_obs = torch.from_numpy(val_obs_np).to(dtype=torch.float16)\n",
    "        del val_obs_np\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Converting NumPy arrays to PyTorch tensors with float16 precision (3/4)...\")\n",
    "        train_targets = torch.from_numpy(train_targets_np).to(dtype=torch.float16)\n",
    "        del train_targets_np\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Converting NumPy arrays to PyTorch tensors with float16 precision (4/4)...\")\n",
    "        val_targets = torch.from_numpy(val_targets_np).to(dtype=torch.float16)\n",
    "        del val_targets_np\n",
    "        gc.collect()\n",
    "\n",
    "        # Create TensorDatasets for use with PyTorch DataLoaders\n",
    "        train_dataset = TensorDataset(train_obs, train_targets)\n",
    "        val_dataset = TensorDataset(val_obs, val_targets)\n",
    "        \n",
    "        print(\"Dataset preparation complete.\")\n",
    "        return train_dataset, val_dataset\n",
    "    except Exception as e:\n",
    "        print(f\"could not load data: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e9e13ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:36:28.179713Z",
     "iopub.status.busy": "2025-07-09T12:36:28.179383Z",
     "iopub.status.idle": "2025-07-09T12:36:28.186447Z",
     "shell.execute_reply": "2025-07-09T12:36:28.185432Z"
    },
    "papermill": {
     "duration": 0.014946,
     "end_time": "2025-07-09T12:36:28.188311",
     "exception": false,
     "start_time": "2025-07-09T12:36:28.173365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_weights_only(filepath):\n",
    "    \"\"\"\n",
    "    Loads only the weights (state_dict) from a .pth file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the .pth file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The state dictionary containing the model weights, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the entire checkpoint\n",
    "        checkpoint = torch.load(filepath, map_location=\"cpu\")\n",
    "\n",
    "        # Check if the loaded object is a state_dict directly\n",
    "        if isinstance(checkpoint, dict) and all(isinstance(k, str) for k in checkpoint.keys()):\n",
    "            # Assume it's a state_dict if all keys are strings (common for weights)\n",
    "            print(f\"Successfully loaded state_dict from {filepath}\")\n",
    "            state_dict = checkpoint\n",
    "        elif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n",
    "            # If it's a dictionary containing a 'state_dict' key (common for full checkpoints)\n",
    "            print(f\"Successfully loaded 'state_dict' from checkpoint in {filepath}\")\n",
    "            state_dict = checkpoint['state_dict']\n",
    "        else:\n",
    "            print(f\"Warning: The .pth file at {filepath} does not seem to contain a standard state_dict or a checkpoint with 'state_dict'.\")\n",
    "            print(\"Attempting to return the loaded object directly. Please inspect its content.\")\n",
    "            state_dict = checkpoint\n",
    "\n",
    "        new_state_dict = {}\n",
    "        for key, value in state_dict.items():\n",
    "            if key.startswith('_orig_mod.'):\n",
    "                # Remove the prefix and add the key-value pair to the new dict\n",
    "                new_key = key[len('_orig_mod.'):]\n",
    "                new_state_dict[new_key] = value\n",
    "            else:\n",
    "                # Keep keys that don't have the prefix as they are\n",
    "                new_state_dict[key] = value\n",
    "        return new_state_dict\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "428fc888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:\n",
      "Connect4ModelVit(\n",
      "  (conv_extractor): Sequential(\n",
      "    (0): InitialExtractor(\n",
      "      (convs): Sequential(\n",
      "        (0): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): ConnectFourBlock(\n",
      "      (conv_next): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "        (1): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): SiLU(inplace=True)\n",
      "        (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (proj_up): Sequential(\n",
      "        (0): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (grouped_0): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)\n",
      "        (1): Grouped1x1SumConv(\n",
      "          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), groups=4)\n",
      "        )\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "      (grouped_1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)\n",
      "        (1): Grouped1x1SumConv(\n",
      "          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), groups=4)\n",
      "        )\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "      (proj_down): Sequential(\n",
      "        (0): SEBlock(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc_scale): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "            (1): SiLU(inplace=True)\n",
      "            (2): Linear(in_features=32, out_features=128, bias=True)\n",
      "            (3): Sigmoid()\n",
      "          )\n",
      "          (fc_offset): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "            (1): SiLU(inplace=True)\n",
      "            (2): Linear(in_features=32, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (2): ConnectFourBlock(\n",
      "      (conv_next): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "        (1): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): SiLU(inplace=True)\n",
      "        (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (proj_up): Sequential(\n",
      "        (0): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (grouped_0): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)\n",
      "        (1): Grouped1x1SumConv(\n",
      "          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), groups=4)\n",
      "        )\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "      (grouped_1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)\n",
      "        (1): Grouped1x1SumConv(\n",
      "          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), groups=4)\n",
      "        )\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "      (proj_down): Sequential(\n",
      "        (0): SEBlock(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc_scale): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "            (1): SiLU(inplace=True)\n",
      "            (2): Linear(in_features=32, out_features=128, bias=True)\n",
      "            (3): Sigmoid()\n",
      "          )\n",
      "          (fc_offset): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "            (1): SiLU(inplace=True)\n",
      "            (2): Linear(in_features=32, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (3): ConnectFourBlock(\n",
      "      (conv_next): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "        (1): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): SiLU(inplace=True)\n",
      "        (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (proj_up): Sequential(\n",
      "        (0): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (grouped_0): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)\n",
      "        (1): Grouped1x1SumConv(\n",
      "          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), groups=4)\n",
      "        )\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "      (grouped_1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)\n",
      "        (1): Grouped1x1SumConv(\n",
      "          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), groups=4)\n",
      "        )\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "      (proj_down): Sequential(\n",
      "        (0): SEBlock(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc_scale): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "            (1): SiLU(inplace=True)\n",
      "            (2): Linear(in_features=32, out_features=128, bias=True)\n",
      "            (3): Sigmoid()\n",
      "          )\n",
      "          (fc_offset): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "            (1): SiLU(inplace=True)\n",
      "            (2): Linear(in_features=32, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (4): ConnectFourBlock(\n",
      "      (conv_next): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "        (1): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): SiLU(inplace=True)\n",
      "        (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (proj_up): Sequential(\n",
      "        (0): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (grouped_0): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)\n",
      "        (1): Grouped1x1SumConv(\n",
      "          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), groups=4)\n",
      "        )\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "      (grouped_1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)\n",
      "        (1): Grouped1x1SumConv(\n",
      "          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), groups=4)\n",
      "        )\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "      (proj_down): Sequential(\n",
      "        (0): SEBlock(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc_scale): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "            (1): SiLU(inplace=True)\n",
      "            (2): Linear(in_features=32, out_features=128, bias=True)\n",
      "            (3): Sigmoid()\n",
      "          )\n",
      "          (fc_offset): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "            (1): SiLU(inplace=True)\n",
      "            (2): Linear(in_features=32, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (5): ConnectFourBlock(\n",
      "      (conv_next): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "        (1): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): SiLU(inplace=True)\n",
      "        (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (proj_up): Sequential(\n",
      "        (0): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (grouped_0): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)\n",
      "        (1): Grouped1x1SumConv(\n",
      "          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), groups=4)\n",
      "        )\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "      (grouped_1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)\n",
      "        (1): Grouped1x1SumConv(\n",
      "          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), groups=4)\n",
      "        )\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "      (proj_down): Sequential(\n",
      "        (0): SEBlock(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc_scale): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "            (1): SiLU(inplace=True)\n",
      "            (2): Linear(in_features=32, out_features=128, bias=True)\n",
      "            (3): Sigmoid()\n",
      "          )\n",
      "          (fc_offset): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "            (1): SiLU(inplace=True)\n",
      "            (2): Linear(in_features=32, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer): VisionTransformerWithTasks(\n",
      "    (proj): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (pos_embed): Embedding(57, 256)\n",
      "    (transformer): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-4): 5 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output_head): Conv1d(3840, 15, kernel_size=(1,), stride=(1,), groups=15)\n",
      "  )\n",
      ")\n",
      "Module                                FLOP    % Total\n",
      "--------------------------------  --------  ---------\n",
      "Connect4ModelVit                  532.590M    100.00%\n",
      " - aten.convolution                66.149M     12.42%\n",
      " - aten.addmm                     448.430M     84.20%\n",
      " - aten.mm                          1.376M      0.26%\n",
      " - aten.bmm                        16.635M      3.12%\n",
      " Connect4ModelVit.conv_extractor   66.305M     12.45%\n",
      "  - aten.convolution               66.141M     12.42%\n",
      "  - aten.addmm                      0.164M      0.03%\n",
      " Connect4ModelVit.transformer     466.285M     87.55%\n",
      "  - aten.mm                         1.376M      0.26%\n",
      "  - aten.addmm                    448.266M     84.17%\n",
      "  - aten.bmm                       16.635M      3.12%\n",
      "  - aten.convolution                0.008M      0.00%\n",
      "Total FLOPS: 532589824\n",
      "Dummy Input Shape: torch.Size([1, 2, 6, 7])\n",
      "Dummy Output: (tensor([[ 0.5047,  0.0647,  0.1024,  0.0737,  0.0598,  0.7668, -0.1699]],\n",
      "       grad_fn=<SplitWithSizesBackward0>), tensor([[0.0348]], grad_fn=<SplitWithSizesBackward0>), tensor([[ 0.6885, -0.1905, -0.1821,  0.0436,  0.3198, -0.3267, -0.0765]],\n",
      "       grad_fn=<SplitWithSizesBackward0>))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nr/z56dzld12zx7qpfvb8j9cjf80000gn/T/ipykernel_6642/3877774015.py:111: UserWarning: mods argument is not needed anymore, you can stop passing it\n",
      "  with FlopCounterMode(model) as count:\n"
     ]
    }
   ],
   "source": [
    "arch = \"vit_medium\"\n",
    "num_run = 0\n",
    "\n",
    "model = get_custom_model(arch)\n",
    "count_model_flops(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "de4248d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./data/c4_data_enriched.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b212c469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset with mmap_mode='r'...\n",
      "Converting NumPy arrays to PyTorch tensors with float16 precision (1/4)...\n",
      "Converting NumPy arrays to PyTorch tensors with float16 precision (2/4)...\n",
      "Converting NumPy arrays to PyTorch tensors with float16 precision (3/4)...\n",
      "Converting NumPy arrays to PyTorch tensors with float16 precision (4/4)...\n",
      "Dataset preparation complete.\n"
     ]
    }
   ],
   "source": [
    "dataset_splits = load_dataset(dataset_path)\n",
    "if dataset_splits is None:\n",
    "    os._exit(1)\n",
    "\n",
    "train_dataset, val_dataset = dataset_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a34d8adc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:36:28.199147Z",
     "iopub.status.busy": "2025-07-09T12:36:28.198693Z",
     "iopub.status.idle": "2025-07-09T12:36:34.229830Z",
     "shell.execute_reply": "2025-07-09T12:36:34.228550Z"
    },
    "papermill": {
     "duration": 6.038598,
     "end_time": "2025-07-09T12:36:34.231422",
     "exception": true,
     "start_time": "2025-07-09T12:36:28.192824",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded state_dict from checkpoints/vit_medium_r0_starting_checkpoint.pth\n",
      "Keys in loaded weights: dict_keys(['conv_extractor.0.convs.0.weight', 'conv_extractor.0.convs.0.bias', 'conv_extractor.0.convs.0.running_mean', 'conv_extractor.0.convs.0.running_var', 'conv_extractor.0.convs.0.num_batches_tracked', 'conv_extractor.0.convs.1.weight', 'conv_extractor.1.conv_next.0.weight', 'conv_extractor.1.conv_next.1.weight', 'conv_extractor.1.conv_next.1.bias', 'conv_extractor.1.conv_next.2.weight', 'conv_extractor.1.conv_next.4.weight', 'conv_extractor.1.proj_up.0.weight', 'conv_extractor.1.proj_up.0.bias', 'conv_extractor.1.proj_up.1.weight', 'conv_extractor.1.grouped_0.0.weight', 'conv_extractor.1.grouped_0.1.conv.weight', 'conv_extractor.1.grouped_0.1.conv.bias', 'conv_extractor.1.grouped_1.0.weight', 'conv_extractor.1.grouped_1.1.conv.weight', 'conv_extractor.1.grouped_1.1.conv.bias', 'conv_extractor.1.proj_down.0.fc_scale.0.weight', 'conv_extractor.1.proj_down.0.fc_scale.0.bias', 'conv_extractor.1.proj_down.0.fc_scale.2.weight', 'conv_extractor.1.proj_down.0.fc_scale.2.bias', 'conv_extractor.1.proj_down.0.fc_offset.0.weight', 'conv_extractor.1.proj_down.0.fc_offset.0.bias', 'conv_extractor.1.proj_down.0.fc_offset.2.weight', 'conv_extractor.1.proj_down.0.fc_offset.2.bias', 'conv_extractor.1.proj_down.1.weight', 'conv_extractor.2.conv_next.0.weight', 'conv_extractor.2.conv_next.1.weight', 'conv_extractor.2.conv_next.1.bias', 'conv_extractor.2.conv_next.2.weight', 'conv_extractor.2.conv_next.4.weight', 'conv_extractor.2.proj_up.0.weight', 'conv_extractor.2.proj_up.0.bias', 'conv_extractor.2.proj_up.1.weight', 'conv_extractor.2.grouped_0.0.weight', 'conv_extractor.2.grouped_0.1.conv.weight', 'conv_extractor.2.grouped_0.1.conv.bias', 'conv_extractor.2.grouped_1.0.weight', 'conv_extractor.2.grouped_1.1.conv.weight', 'conv_extractor.2.grouped_1.1.conv.bias', 'conv_extractor.2.proj_down.0.fc_scale.0.weight', 'conv_extractor.2.proj_down.0.fc_scale.0.bias', 'conv_extractor.2.proj_down.0.fc_scale.2.weight', 'conv_extractor.2.proj_down.0.fc_scale.2.bias', 'conv_extractor.2.proj_down.0.fc_offset.0.weight', 'conv_extractor.2.proj_down.0.fc_offset.0.bias', 'conv_extractor.2.proj_down.0.fc_offset.2.weight', 'conv_extractor.2.proj_down.0.fc_offset.2.bias', 'conv_extractor.2.proj_down.1.weight', 'conv_extractor.3.conv_next.0.weight', 'conv_extractor.3.conv_next.1.weight', 'conv_extractor.3.conv_next.1.bias', 'conv_extractor.3.conv_next.2.weight', 'conv_extractor.3.conv_next.4.weight', 'conv_extractor.3.proj_up.0.weight', 'conv_extractor.3.proj_up.0.bias', 'conv_extractor.3.proj_up.1.weight', 'conv_extractor.3.grouped_0.0.weight', 'conv_extractor.3.grouped_0.1.conv.weight', 'conv_extractor.3.grouped_0.1.conv.bias', 'conv_extractor.3.grouped_1.0.weight', 'conv_extractor.3.grouped_1.1.conv.weight', 'conv_extractor.3.grouped_1.1.conv.bias', 'conv_extractor.3.proj_down.0.fc_scale.0.weight', 'conv_extractor.3.proj_down.0.fc_scale.0.bias', 'conv_extractor.3.proj_down.0.fc_scale.2.weight', 'conv_extractor.3.proj_down.0.fc_scale.2.bias', 'conv_extractor.3.proj_down.0.fc_offset.0.weight', 'conv_extractor.3.proj_down.0.fc_offset.0.bias', 'conv_extractor.3.proj_down.0.fc_offset.2.weight', 'conv_extractor.3.proj_down.0.fc_offset.2.bias', 'conv_extractor.3.proj_down.1.weight', 'conv_extractor.4.conv_next.0.weight', 'conv_extractor.4.conv_next.1.weight', 'conv_extractor.4.conv_next.1.bias', 'conv_extractor.4.conv_next.2.weight', 'conv_extractor.4.conv_next.4.weight', 'conv_extractor.4.proj_up.0.weight', 'conv_extractor.4.proj_up.0.bias', 'conv_extractor.4.proj_up.1.weight', 'conv_extractor.4.grouped_0.0.weight', 'conv_extractor.4.grouped_0.1.conv.weight', 'conv_extractor.4.grouped_0.1.conv.bias', 'conv_extractor.4.grouped_1.0.weight', 'conv_extractor.4.grouped_1.1.conv.weight', 'conv_extractor.4.grouped_1.1.conv.bias', 'conv_extractor.4.proj_down.0.fc_scale.0.weight', 'conv_extractor.4.proj_down.0.fc_scale.0.bias', 'conv_extractor.4.proj_down.0.fc_scale.2.weight', 'conv_extractor.4.proj_down.0.fc_scale.2.bias', 'conv_extractor.4.proj_down.0.fc_offset.0.weight', 'conv_extractor.4.proj_down.0.fc_offset.0.bias', 'conv_extractor.4.proj_down.0.fc_offset.2.weight', 'conv_extractor.4.proj_down.0.fc_offset.2.bias', 'conv_extractor.4.proj_down.1.weight', 'conv_extractor.5.conv_next.0.weight', 'conv_extractor.5.conv_next.1.weight', 'conv_extractor.5.conv_next.1.bias', 'conv_extractor.5.conv_next.2.weight', 'conv_extractor.5.conv_next.4.weight', 'conv_extractor.5.proj_up.0.weight', 'conv_extractor.5.proj_up.0.bias', 'conv_extractor.5.proj_up.1.weight', 'conv_extractor.5.grouped_0.0.weight', 'conv_extractor.5.grouped_0.1.conv.weight', 'conv_extractor.5.grouped_0.1.conv.bias', 'conv_extractor.5.grouped_1.0.weight', 'conv_extractor.5.grouped_1.1.conv.weight', 'conv_extractor.5.grouped_1.1.conv.bias', 'conv_extractor.5.proj_down.0.fc_scale.0.weight', 'conv_extractor.5.proj_down.0.fc_scale.0.bias', 'conv_extractor.5.proj_down.0.fc_scale.2.weight', 'conv_extractor.5.proj_down.0.fc_scale.2.bias', 'conv_extractor.5.proj_down.0.fc_offset.0.weight', 'conv_extractor.5.proj_down.0.fc_offset.0.bias', 'conv_extractor.5.proj_down.0.fc_offset.2.weight', 'conv_extractor.5.proj_down.0.fc_offset.2.bias', 'conv_extractor.5.proj_down.1.weight', 'transformer.task_tokens', 'transformer.proj.weight', 'transformer.proj.bias', 'transformer.pos_embed.weight', 'transformer.transformer.layers.0.self_attn.in_proj_weight', 'transformer.transformer.layers.0.self_attn.in_proj_bias', 'transformer.transformer.layers.0.self_attn.out_proj.weight', 'transformer.transformer.layers.0.self_attn.out_proj.bias', 'transformer.transformer.layers.0.linear1.weight', 'transformer.transformer.layers.0.linear1.bias', 'transformer.transformer.layers.0.linear2.weight', 'transformer.transformer.layers.0.linear2.bias', 'transformer.transformer.layers.0.norm1.weight', 'transformer.transformer.layers.0.norm1.bias', 'transformer.transformer.layers.0.norm2.weight', 'transformer.transformer.layers.0.norm2.bias', 'transformer.transformer.layers.1.self_attn.in_proj_weight', 'transformer.transformer.layers.1.self_attn.in_proj_bias', 'transformer.transformer.layers.1.self_attn.out_proj.weight', 'transformer.transformer.layers.1.self_attn.out_proj.bias', 'transformer.transformer.layers.1.linear1.weight', 'transformer.transformer.layers.1.linear1.bias', 'transformer.transformer.layers.1.linear2.weight', 'transformer.transformer.layers.1.linear2.bias', 'transformer.transformer.layers.1.norm1.weight', 'transformer.transformer.layers.1.norm1.bias', 'transformer.transformer.layers.1.norm2.weight', 'transformer.transformer.layers.1.norm2.bias', 'transformer.transformer.layers.2.self_attn.in_proj_weight', 'transformer.transformer.layers.2.self_attn.in_proj_bias', 'transformer.transformer.layers.2.self_attn.out_proj.weight', 'transformer.transformer.layers.2.self_attn.out_proj.bias', 'transformer.transformer.layers.2.linear1.weight', 'transformer.transformer.layers.2.linear1.bias', 'transformer.transformer.layers.2.linear2.weight', 'transformer.transformer.layers.2.linear2.bias', 'transformer.transformer.layers.2.norm1.weight', 'transformer.transformer.layers.2.norm1.bias', 'transformer.transformer.layers.2.norm2.weight', 'transformer.transformer.layers.2.norm2.bias', 'transformer.transformer.layers.3.self_attn.in_proj_weight', 'transformer.transformer.layers.3.self_attn.in_proj_bias', 'transformer.transformer.layers.3.self_attn.out_proj.weight', 'transformer.transformer.layers.3.self_attn.out_proj.bias', 'transformer.transformer.layers.3.linear1.weight', 'transformer.transformer.layers.3.linear1.bias', 'transformer.transformer.layers.3.linear2.weight', 'transformer.transformer.layers.3.linear2.bias', 'transformer.transformer.layers.3.norm1.weight', 'transformer.transformer.layers.3.norm1.bias', 'transformer.transformer.layers.3.norm2.weight', 'transformer.transformer.layers.3.norm2.bias', 'transformer.transformer.layers.4.self_attn.in_proj_weight', 'transformer.transformer.layers.4.self_attn.in_proj_bias', 'transformer.transformer.layers.4.self_attn.out_proj.weight', 'transformer.transformer.layers.4.self_attn.out_proj.bias', 'transformer.transformer.layers.4.linear1.weight', 'transformer.transformer.layers.4.linear1.bias', 'transformer.transformer.layers.4.linear2.weight', 'transformer.transformer.layers.4.linear2.bias', 'transformer.transformer.layers.4.norm1.weight', 'transformer.transformer.layers.4.norm1.bias', 'transformer.transformer.layers.4.norm2.weight', 'transformer.transformer.layers.4.norm2.bias', 'transformer.output_head.weight', 'transformer.output_head.bias'])\n",
      "Successfully loaded weights into a new model.\n"
     ]
    }
   ],
   "source": [
    "save_model_name = f\"checkpoints/{arch}_r{num_run}\"\n",
    "checkpoint_file = f\"{save_model_name}_checkpoint.pth\"\n",
    "best_model_file = f\"{save_model_name}_best_model.pth\"\n",
    "\n",
    "load_file = f\"{save_model_name}_starting_checkpoint.pth\"\n",
    "\n",
    "weights = load_weights_only(load_file)\n",
    "if weights:\n",
    "    print(\"Keys in loaded weights:\", weights.keys())\n",
    "    try:\n",
    "        model.load_state_dict(weights)\n",
    "        print(\"Successfully loaded weights into a new model.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load weights into a new model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4acb1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "# INFO: epochs and batches decreased for presentation\n",
    "args = dict(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    model=model,\n",
    "    batch_size=2 ** 11,\n",
    "    learning_rate=5e-5,\n",
    "    epochs=4,\n",
    "    num_batches_per_epoch=20,\n",
    "    warmup_fraction=0.2,\n",
    "    weight_decay=4e-5,\n",
    "    checkpoint_path=checkpoint_file,\n",
    "    best_model_path=best_model_file,\n",
    "    device=device,\n",
    "    use_tqdm=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d5587e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/57415 [00:00<?, ?it/s]/opt/homebrew/anaconda3/envs/timm/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin epoch 0 with LR: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/57415 [00:11<76:23:22,  4.79s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/4], Step [1 / 20], Running Loss: {'combined': 6455.566162109375, 'policy': 0.4191662669181824, 'value': 1520.9827880859375, 'next_value': 7429.206298828125}, LR: 0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/57415 [00:13<40:13:39,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/4], Step [3 / 20], Running Loss: {'combined': 6245.2410888671875, 'policy': 0.39965417236089706, 'value': 1481.2819519042969, 'next_value': 7674.1729736328125}, LR: 0.000013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/57415 [00:16<29:20:34,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/4], Step [5 / 20], Running Loss: {'combined': 6095.862060546875, 'policy': 0.382262443502744, 'value': 1484.161844889323, 'next_value': 7890.757405598958}, LR: 0.000019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/57415 [00:19<25:04:39,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/4], Step [7 / 20], Running Loss: {'combined': 5811.6326904296875, 'policy': 0.3578588180243969, 'value': 1436.7593078613281, 'next_value': 7962.851989746094}, LR: 0.000025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/57415 [00:21<23:12:10,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/4], Step [9 / 20], Running Loss: {'combined': 5620.75244140625, 'policy': 0.34198285937309264, 'value': 1392.0317138671876, 'next_value': 8088.920849609375}, LR: 0.000031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/57415 [00:24<22:22:17,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/4], Step [11 / 20], Running Loss: {'combined': 5458.125406901042, 'policy': 0.3294528548916181, 'value': 1349.6328430175781, 'next_value': 8139.6396891276045}, LR: 0.000038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/57415 [00:27<22:10:26,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/4], Step [13 / 20], Running Loss: {'combined': 5347.727992466518, 'policy': 0.3197121428591864, 'value': 1343.777596609933, 'next_value': 8068.289202008928}, LR: 0.000044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/57415 [00:29<21:32:40,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/4], Step [15 / 20], Running Loss: {'combined': 5206.093719482422, 'policy': 0.3088137209415436, 'value': 1309.4091300964355, 'next_value': 8085.4735107421875}, LR: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/57415 [00:32<21:17:38,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/4], Step [17 / 20], Running Loss: {'combined': 5094.994656032986, 'policy': 0.3000914032260577, 'value': 1290.1588711208767, 'next_value': 8039.217122395833}, LR: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/57415 [00:35<21:13:47,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/4], Step [19 / 20], Running Loss: {'combined': 5011.550329589843, 'policy': 0.29277342185378075, 'value': 1280.660922241211, 'next_value': 8031.551171875}, LR: 0.000050\n",
      "Epoch [0/4] done, Training Loss: {'combined': 5011.550329589843, 'policy': 0.29277342185378075, 'value': 1280.660922241211, 'next_value': 8031.551171875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:07,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: {'combined': 1218.2769470214844, 'policy': 0.08711761981248856, 'value': 271.2638626098633, 'next_value': 758.3688659667969}\n",
      "Validation Policy Acc: {'strong': tensor(0.9098, device='mps:0'), 'weak': tensor(0.9816, device='mps:0'), 'strong_nv': tensor(0.8337, device='mps:0'), 'weak_nv': tensor(0.9885, device='mps:0')}\n",
      "Validation accuracy improved. Saved best model to checkpoints/vit_medium_r0_best_model.pth\n",
      "Begin epoch 1 with LR: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 22/57415 [00:46<48:25:13,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [1 / 20], Running Loss: {'combined': 1886.5298461914062, 'policy': 0.1216006949543953, 'value': 575.4322509765625, 'next_value': 950.9069519042969}, LR: 0.000049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 24/57415 [00:48<33:19:44,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [3 / 20], Running Loss: {'combined': 1934.5039367675781, 'policy': 0.12195104360580444, 'value': 618.8231048583984, 'next_value': 961.7043762207031}, LR: 0.000048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 26/57415 [00:50<25:56:58,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [5 / 20], Running Loss: {'combined': 1871.9553629557292, 'policy': 0.11906309922536214, 'value': 585.7340342203776, 'next_value': 955.9037373860677}, LR: 0.000047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 28/57415 [00:53<22:28:31,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [7 / 20], Running Loss: {'combined': 1824.9727478027344, 'policy': 0.11698203813284636, 'value': 560.9809913635254, 'next_value': 941.7138595581055}, LR: 0.000046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 30/57415 [00:55<20:46:15,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [9 / 20], Running Loss: {'combined': 1821.641748046875, 'policy': 0.11582648903131484, 'value': 568.7159576416016, 'next_value': 946.6091430664062}, LR: 0.000044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 32/57415 [00:58<19:47:44,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [11 / 20], Running Loss: {'combined': 1790.8223266601562, 'policy': 0.11394132611652215, 'value': 556.6498235066732, 'next_value': 947.5925750732422}, LR: 0.000043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 34/57415 [01:00<19:23:23,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [13 / 20], Running Loss: {'combined': 1765.6448887416295, 'policy': 0.11199862137436867, 'value': 550.7519792829241, 'next_value': 949.0671648297991}, LR: 0.000041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 36/57415 [01:02<19:11:04,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [15 / 20], Running Loss: {'combined': 1750.298484802246, 'policy': 0.11065924493595958, 'value': 548.6295166015625, 'next_value': 950.7653884887695}, LR: 0.000039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 38/57415 [01:05<19:02:36,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [17 / 20], Running Loss: {'combined': 1746.952880859375, 'policy': 0.1100816097524431, 'value': 550.1558295355903, 'next_value': 959.8096516927084}, LR: 0.000037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 40/57415 [01:07<18:57:57,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [19 / 20], Running Loss: {'combined': 1724.4182739257812, 'policy': 0.10846277512609959, 'value': 543.7958801269531, 'next_value': 959.946499633789}, LR: 0.000035\n",
      "Epoch [1/4] done, Training Loss: {'combined': 1724.4182739257812, 'policy': 0.10846277512609959, 'value': 543.7958801269531, 'next_value': 959.946499633789}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:07,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: {'combined': 834.0098815917969, 'policy': 0.05389855224639177, 'value': 226.0476448059082, 'next_value': 689.7672393798828}\n",
      "Validation Policy Acc: {'strong': tensor(0.9270, device='mps:0'), 'weak': tensor(0.9950, device='mps:0'), 'strong_nv': tensor(0.8430, device='mps:0'), 'weak_nv': tensor(0.9941, device='mps:0')}\n",
      "Validation accuracy improved. Saved best model to checkpoints/vit_medium_r0_best_model.pth\n",
      "Begin epoch 2 with LR: 0.000035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 42/57415 [01:17<43:07:16,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4], Step [1 / 20], Running Loss: {'combined': 1648.7371215820312, 'policy': 0.09573324397206306, 'value': 600.3723449707031, 'next_value': 910.3231506347656}, LR: 0.000032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 44/57415 [01:19<30:48:15,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4], Step [3 / 20], Running Loss: {'combined': 1584.0894470214844, 'policy': 0.09331063367426395, 'value': 560.5438232421875, 'next_value': 904.3923950195312}, LR: 0.000030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 46/57415 [01:21<24:45:25,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4], Step [5 / 20], Running Loss: {'combined': 1493.5968017578125, 'policy': 0.08988862484693527, 'value': 505.5275624593099, 'next_value': 891.8297729492188}, LR: 0.000027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 48/57415 [01:24<22:11:43,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4], Step [7 / 20], Running Loss: {'combined': 1497.7239990234375, 'policy': 0.0896861394867301, 'value': 511.93869400024414, 'next_value': 889.2390899658203}, LR: 0.000025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 50/57415 [01:26<20:48:13,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4], Step [9 / 20], Running Loss: {'combined': 1500.903271484375, 'policy': 0.08989177271723747, 'value': 513.3483184814453, 'next_value': 886.37216796875}, LR: 0.000023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 52/57415 [01:29<19:54:39,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4], Step [11 / 20], Running Loss: {'combined': 1509.890645345052, 'policy': 0.090319716061155, 'value': 518.8029708862305, 'next_value': 878.9049580891927}, LR: 0.000020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 54/57415 [01:31<19:32:37,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4], Step [13 / 20], Running Loss: {'combined': 1501.8181326729912, 'policy': 0.09045373914497239, 'value': 509.89683750697543, 'next_value': 873.8389238630023}, LR: 0.000018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 56/57415 [01:34<19:23:47,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4], Step [15 / 20], Running Loss: {'combined': 1499.010871887207, 'policy': 0.09047550149261951, 'value': 507.54509353637695, 'next_value': 867.1075439453125}, LR: 0.000015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 58/57415 [01:36<19:20:50,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4], Step [17 / 20], Running Loss: {'combined': 1494.731913248698, 'policy': 0.09003447575701608, 'value': 508.4146033393012, 'next_value': 859.7254876030815}, LR: 0.000013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 60/57415 [01:38<19:14:44,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4], Step [19 / 20], Running Loss: {'combined': 1484.5274963378906, 'policy': 0.08927823938429355, 'value': 507.032698059082, 'next_value': 847.1240142822265}, LR: 0.000011\n",
      "Epoch [2/4] done, Training Loss: {'combined': 1484.5274963378906, 'policy': 0.08927823938429355, 'value': 507.032698059082, 'next_value': 847.1240142822265}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:07,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: {'combined': 752.4904907226562, 'policy': 0.04819699060171843, 'value': 213.56237182617187, 'next_value': 569.5821685791016}\n",
      "Validation Policy Acc: {'strong': tensor(0.9345, device='mps:0'), 'weak': tensor(0.9957, device='mps:0'), 'strong_nv': tensor(0.8254, device='mps:0'), 'weak_nv': tensor(0.9946, device='mps:0')}\n",
      "Validation accuracy improved. Saved best model to checkpoints/vit_medium_r0_best_model.pth\n",
      "Begin epoch 3 with LR: 0.000011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 62/57415 [01:48<45:01:50,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/4], Step [1 / 20], Running Loss: {'combined': 1452.849853515625, 'policy': 0.08292215317487717, 'value': 544.6606140136719, 'next_value': 789.6776733398438}, LR: 0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 64/57415 [01:51<32:03:06,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/4], Step [3 / 20], Running Loss: {'combined': 1437.3251953125, 'policy': 0.08382716774940491, 'value': 518.9060440063477, 'next_value': 801.4747924804688}, LR: 0.000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 66/57415 [01:53<25:39:14,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/4], Step [5 / 20], Running Loss: {'combined': 1424.0284220377605, 'policy': 0.08342431982358296, 'value': 510.47133382161456, 'next_value': 793.1387023925781}, LR: 0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 68/57415 [01:56<22:30:13,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/4], Step [7 / 20], Running Loss: {'combined': 1429.1421813964844, 'policy': 0.08429753594100475, 'value': 506.36793518066406, 'next_value': 797.9889068603516}, LR: 0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 70/57415 [01:58<20:57:54,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/4], Step [9 / 20], Running Loss: {'combined': 1420.0154052734374, 'policy': 0.08433568179607391, 'value': 496.26243591308594, 'next_value': 803.9615234375}, LR: 0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 72/57415 [02:01<20:12:47,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/4], Step [11 / 20], Running Loss: {'combined': 1414.1031799316406, 'policy': 0.08402304723858833, 'value': 494.00375111897785, 'next_value': 798.6896362304688}, LR: 0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 74/57415 [02:03<19:51:52,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/4], Step [13 / 20], Running Loss: {'combined': 1393.6715262276787, 'policy': 0.08333131085549082, 'value': 480.73399353027344, 'next_value': 796.2442932128906}, LR: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 76/57415 [02:06<19:44:59,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/4], Step [15 / 20], Running Loss: {'combined': 1391.1943893432617, 'policy': 0.08350273361429572, 'value': 476.3062381744385, 'next_value': 798.6081924438477}, LR: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 78/57415 [02:08<19:55:59,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/4], Step [17 / 20], Running Loss: {'combined': 1398.4647759331597, 'policy': 0.0836206976738241, 'value': 482.51224941677515, 'next_value': 797.4556206597222}, LR: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 80/57415 [02:11<19:44:16,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/4], Step [19 / 20], Running Loss: {'combined': 1389.3052795410156, 'policy': 0.08318107016384602, 'value': 477.8706939697266, 'next_value': 796.2389831542969}, LR: 0.000000\n",
      "Epoch [3/4] done, Training Loss: {'combined': 1389.3052795410156, 'policy': 0.08318107016384602, 'value': 477.8706939697266, 'next_value': 796.2389831542969}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:07,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: {'combined': 746.3404388427734, 'policy': 0.047507242672145365, 'value': 215.40302581787108, 'next_value': 558.6499664306641}\n",
      "Validation Policy Acc: {'strong': tensor(0.9353, device='mps:0'), 'weak': tensor(0.9959, device='mps:0'), 'strong_nv': tensor(0.8243, device='mps:0'), 'weak_nv': tensor(0.9947, device='mps:0')}\n",
      "Validation accuracy improved. Saved best model to checkpoints/vit_medium_r0_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 80/57415 [02:19<27:48:13,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n",
      "Best validation acc achieved: 0.9353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05efe9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mine hard examples\n",
    "if False:\n",
    "    model.to(device)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    hard_examples = validate_model(model, train_loader, device, max_batches=100, use_tqdm=True, mine_hard=True)\n",
    "    if hard_examples:\n",
    "        print(len(hard_examples[0]))\n",
    "        hard_inputs, hard_targets, hard_losses = hard_examples\n",
    "        hard_examples_tensor = {\n",
    "            \"obs\": hard_inputs.cpu(),\n",
    "            \"targets\": hard_targets.cpu(),\n",
    "            \"loss\": hard_losses.cpu()\n",
    "        }\n",
    "\n",
    "        torch.save(hard_examples_tensor, \"hard_examples.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "76fdcd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataset\n",
    "del val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e772f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = np.load(dataset_path, mmap_mode='r')\n",
    "test_obs = torch.from_numpy(dataset[\"x_test\"])\n",
    "test_targets = torch.from_numpy(dataset[\"y_test\"])\n",
    "\n",
    "random_seed = 534984\n",
    "indices = np.arange(len(test_obs))\n",
    "np.random.seed(random_seed)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "test_obs = test_obs[indices]\n",
    "test_targets = test_targets[indices]\n",
    "test_dataset = TensorDataset(test_obs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ce67b297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [01:37,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: {'combined': 741.1239390869141, 'policy': 0.046999649949371815, 'value': 215.28080293273925, 'next_value': 558.4663616333008}\n",
      "Validation Policy Acc: {'strong': tensor(0.9348, device='mps:0'), 'weak': tensor(0.9962, device='mps:0'), 'strong_nv': tensor(0.8247, device='mps:0'), 'weak_nv': tensor(0.9951, device='mps:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'strong': tensor(0.9348, device='mps:0'),\n",
       " 'weak': tensor(0.9962, device='mps:0'),\n",
       " 'strong_nv': tensor(0.8247, device='mps:0'),\n",
       " 'weak_nv': tensor(0.9951, device='mps:0')}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False, num_workers=4, pin_memory=True)\n",
    "validate_model(model, test_loader, device, max_batches=500, use_tqdm=True) # INFO: batches decreased for presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "61cc8b66",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2025-07-07T08:21:31.968245Z",
     "iopub.status.idle": "2025-07-07T08:21:31.968772Z",
     "shell.execute_reply": "2025-07-07T08:21:31.96862Z",
     "shell.execute_reply.started": "2025-07-07T08:21:31.968604Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `Connect4ModelVit([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `Connect4ModelVit([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping constant folding for op SequenceEmpty with multiple outputs.\n",
      "Skipping constant folding for op SequenceEmpty with multiple outputs.\n",
      "Skipping constant folding for op SequenceEmpty with multiple outputs.\n",
      "Skipping constant folding for op SequenceEmpty with multiple outputs.\n",
      "Skipping constant folding for op SequenceEmpty with multiple outputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 1 of general pattern rewrite rules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping constant folding for op SequenceEmpty with multiple outputs.\n",
      "Skipping constant folding for op SequenceEmpty with multiple outputs.\n",
      "Skipping constant folding for op SequenceEmpty with multiple outputs.\n",
      "Skipping constant folding for op SequenceEmpty with multiple outputs.\n",
      "Skipping constant folding for op SequenceEmpty with multiple outputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to checkpoints/vit_medium_r0.onnx\n"
     ]
    }
   ],
   "source": [
    "save_model = model\n",
    "save_model.to(torch.device(\"cpu\"))\n",
    "save_model.eval()\n",
    "save_model(torch.randn((1, 2, 6, 7)))\n",
    "\n",
    "example_inputs = (torch.randn(1, 2, 6, 7),)\n",
    "onnx_program = torch.onnx.export(save_model, example_inputs, dynamo=True)\n",
    "onnx_program.save(f\"{save_model_name}.onnx\")\n",
    "print(f\"Model saved to {save_model_name}.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d642b50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 3\n",
      "Sample input: [array([[[[-5.25287628e-01,  6.11568429e-02,  5.17404735e-01,\n",
      "           5.96397400e-01,  3.84037286e-01, -1.59243613e-01,\n",
      "           5.42762220e-01],\n",
      "         [ 2.14245528e-01,  7.87356138e-01, -1.58834112e+00,\n",
      "           1.75697207e-01,  8.48437369e-01, -4.95422870e-01,\n",
      "           2.99675852e-01],\n",
      "         [-3.49775493e-01,  3.61759448e+00, -3.38464499e-01,\n",
      "           4.38362896e-01, -4.41578239e-01,  8.47287774e-01,\n",
      "           6.50815010e-01],\n",
      "         [-8.28610063e-01,  5.07761165e-02,  1.85775951e-01,\n",
      "          -5.23240447e-01, -1.61857158e-01,  9.11434650e-01,\n",
      "          -7.95467079e-01],\n",
      "         [-1.02821314e+00, -1.25863028e+00,  4.69664156e-01,\n",
      "           1.06807184e+00, -7.85515010e-01, -2.44781435e-01,\n",
      "          -2.26164132e-01],\n",
      "         [ 3.58530849e-01,  5.18474758e-01,  1.41793907e-01,\n",
      "           6.62241220e-01, -7.16749191e-01,  8.76955807e-01,\n",
      "          -2.35519099e+00]],\n",
      "\n",
      "        [[-8.96165967e-01,  5.93281925e-01,  1.39522731e+00,\n",
      "          -1.40896976e+00,  5.28469682e-02, -6.62998259e-01,\n",
      "           4.45156157e-01],\n",
      "         [-6.11081086e-02, -6.71671808e-01,  7.24065423e-01,\n",
      "           7.33780742e-01,  1.25089443e+00, -1.67231724e-01,\n",
      "          -1.51135111e+00],\n",
      "         [-5.59243202e-01,  1.79064393e+00,  7.44518936e-01,\n",
      "          -1.26176834e+00, -6.97182715e-01, -5.97416699e-01,\n",
      "          -1.10025704e-01],\n",
      "         [ 2.38390177e-01, -9.02450383e-01, -2.06854805e-01,\n",
      "          -3.73630762e-01,  5.70721328e-01,  3.77921045e-01,\n",
      "          -1.11284471e+00],\n",
      "         [-8.31882477e-01,  7.26195812e-01,  1.11531270e+00,\n",
      "           7.66376495e-01,  9.76191700e-01,  8.85180533e-01,\n",
      "           8.62934801e-04],\n",
      "         [-5.40238202e-01, -7.48371243e-01,  1.24637389e+00,\n",
      "           7.29602695e-01, -5.80591083e-01, -1.01106334e+00,\n",
      "           7.06345588e-02]]]], dtype=float32), array([[[[ 0.6909438 ,  1.652573  ,  0.1870096 , -0.1086788 ,\n",
      "          -0.39115828, -0.5453906 , -0.48717326],\n",
      "         [ 0.4121928 , -0.40136337, -0.34438246, -0.09560257,\n",
      "           0.13371287, -0.24308741,  1.7246919 ],\n",
      "         [-0.4488014 , -0.7536893 , -0.45685795, -0.13770545,\n",
      "          -0.97870296,  0.63237983, -0.627578  ],\n",
      "         [-1.0629995 ,  0.564603  , -1.7189088 , -1.5327575 ,\n",
      "          -0.32386175, -1.3899152 , -1.4792074 ],\n",
      "         [-2.1263409 ,  0.16015717,  0.4288024 , -1.4614489 ,\n",
      "           0.2816987 ,  0.02158937,  0.4901206 ],\n",
      "         [-0.539737  ,  0.3794548 ,  0.8807121 ,  0.2786512 ,\n",
      "           0.73922205,  0.61968654,  0.9433908 ]],\n",
      "\n",
      "        [[-1.4350202 ,  2.237343  ,  1.246271  ,  0.4109951 ,\n",
      "          -1.1356858 , -0.06041432,  0.4448754 ],\n",
      "         [ 0.9016404 ,  0.03440847, -0.8284372 , -1.5373484 ,\n",
      "           1.295879  , -0.21645507, -1.4870741 ],\n",
      "         [ 0.5521031 ,  1.4166483 ,  1.3982335 , -0.36162773,\n",
      "           0.07394682,  1.009749  , -2.921622  ],\n",
      "         [ 0.04790994, -0.4752176 , -0.16738088, -2.400102  ,\n",
      "          -0.69966036, -1.8587266 ,  0.2739172 ],\n",
      "         [-0.05684797,  1.28728   , -0.4405326 , -2.1836894 ,\n",
      "          -0.8429825 , -0.53528017, -0.45093128],\n",
      "         [-2.016722  ,  1.7796152 , -1.1228901 , -1.296547  ,\n",
      "          -0.04375945,  0.52064097, -0.78000516]]]], dtype=float32), array([[[[ 1.70326495e+00,  6.91230074e-02,  3.77136588e-01,\n",
      "           5.32474577e-01,  7.55712807e-01,  1.43889546e+00,\n",
      "           1.11710906e+00],\n",
      "         [-5.47569513e-01,  7.81796277e-01,  5.70446432e-01,\n",
      "          -6.88259780e-01, -4.11819935e-01, -9.43800449e-01,\n",
      "           1.03221619e+00],\n",
      "         [-4.38248932e-01,  1.24111676e+00,  7.42366672e-01,\n",
      "           1.36440527e+00, -9.92301941e-01,  1.26127172e+00,\n",
      "           7.26392388e-01],\n",
      "         [ 2.72871566e+00,  1.08719552e+00, -3.99325937e-01,\n",
      "          -9.97961879e-01,  1.12592638e+00, -2.38733798e-01,\n",
      "           1.11727321e+00],\n",
      "         [ 4.15264726e-01, -1.52378216e-01,  1.24017954e+00,\n",
      "           9.29367721e-01,  6.30405843e-01,  6.77461207e-01,\n",
      "          -4.11152303e-01],\n",
      "         [-9.07726049e-01, -9.48496759e-01,  1.78889596e+00,\n",
      "           4.50382590e-01, -7.41274504e-04, -7.95263126e-02,\n",
      "           1.58729732e+00]],\n",
      "\n",
      "        [[ 1.68901801e-01,  9.80372131e-02,  6.95251584e-01,\n",
      "           1.35951981e-01,  6.21699333e-01,  8.98736268e-02,\n",
      "          -4.86221015e-02],\n",
      "         [ 5.15174985e-01, -7.79146314e-01,  1.07142544e+00,\n",
      "          -1.06989837e+00,  1.32384598e+00, -4.20355886e-01,\n",
      "          -1.05617188e-01],\n",
      "         [ 1.37726903e+00, -1.51169205e+00, -1.17854452e+00,\n",
      "          -5.31300187e-01,  2.97539926e+00,  5.22865176e-01,\n",
      "           3.35806221e-01],\n",
      "         [ 3.48702967e-01, -2.97831208e-01, -1.76382542e+00,\n",
      "          -3.97896051e-01, -1.09902509e-01,  5.22942841e-01,\n",
      "          -2.23030305e+00],\n",
      "         [ 1.38866234e+00,  8.09601128e-01,  9.35124338e-01,\n",
      "          -1.17340779e+00,  2.56800383e-01, -5.28470218e-01,\n",
      "          -2.48294715e-02],\n",
      "         [ 1.53180563e+00, -1.60004866e+00, -4.60523427e-01,\n",
      "          -1.01797685e-01, -7.52280116e-01, -2.21247077e+00,\n",
      "           4.32425052e-01]]]], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[-27.231363 , -20.608765 ,  -6.5820737,  23.44077  ,  -7.0962567,\n",
       "         -18.519863 , -25.27468  ]], dtype=float32),\n",
       " array([[102.15377]], dtype=float32),\n",
       " array([[-98.50069, -93.21299, -88.82956, 103.82227, -88.43998, -96.47022,\n",
       "         -98.75507]], dtype=float32)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(f\"{save_model_name}.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "import onnxruntime\n",
    "\n",
    "example_inputs = (torch.randn(1, 2, 6, 7),torch.randn(1, 2, 6, 7),torch.randn(1, 2, 6, 7),)\n",
    "onnx_inputs = [tensor.numpy(force=True) for tensor in example_inputs]\n",
    "print(f\"Input length: {len(onnx_inputs)}\")\n",
    "print(f\"Sample input: {onnx_inputs}\")\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\n",
    "    f\"./{save_model_name}.onnx\", providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "\n",
    "onnxruntime_input = {input_arg.name: input_value for input_arg, input_value in zip(ort_session.get_inputs(), onnx_inputs)}\n",
    "onnxruntime_outputs = ort_session.run(None, onnxruntime_input)\n",
    "onnxruntime_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78a4704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-27.2314, -20.6088,  -6.5821,  23.4408,  -7.0963, -18.5199, -25.2747]],\n",
       "        grad_fn=<SplitWithSizesBackward0>),\n",
       " tensor([[102.1539]], grad_fn=<SplitWithSizesBackward0>),\n",
       " tensor([[-98.5007, -93.2131, -88.8296, 103.8223, -88.4400, -96.4703, -98.7551]],\n",
       "        grad_fn=<SplitWithSizesBackward0>))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(example_inputs[0])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7289025,
     "sourceId": 11619068,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 378556,
     "modelInstanceId": 357233,
     "sourceId": 438795,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "timm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 211.963565,
   "end_time": "2025-07-09T12:36:39.244338",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-09T12:33:07.280773",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
