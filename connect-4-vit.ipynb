{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d06fbb87",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:13.288746Z",
     "iopub.status.busy": "2025-07-09T12:33:13.288421Z",
     "iopub.status.idle": "2025-07-09T12:33:13.301616Z",
     "shell.execute_reply": "2025-07-09T12:33:13.300177Z"
    },
    "papermill": {
     "duration": 0.021113,
     "end_time": "2025-07-09T12:33:13.303727",
     "exception": false,
     "start_time": "2025-07-09T12:33:13.282614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "\n",
    "def argmax_accuracy(output: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the count of correctly predicted samples for a batch based on\n",
    "    matching the output argmax with *any* of the target argmax indices,\n",
    "    returning the count as a tensor suitable for XLA accumulation.\n",
    "\n",
    "    Args:\n",
    "        output (torch.Tensor): Model output tensor of shape (n, w), where n is\n",
    "                               batch size and w is the vector dimension.\n",
    "        target (torch.Tensor): Target tensor of the same shape (n, w).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A scalar tensor containing the count of correct predictions\n",
    "                      for the batch, located on the same device as inputs.\n",
    "                      Returns a zero tensor if batch size is 0.\n",
    "    \"\"\"\n",
    "    if output.shape != target.shape:\n",
    "        raise ValueError(f\"Output and target tensors must have the same shape. Got {output.shape} and {target.shape}\")\n",
    "    if output.dim() != 2:\n",
    "         raise ValueError(f\"Output and target tensors must be 2D (n, w). Got {output.dim()} dimensions.\")\n",
    "\n",
    "    n, w = output.shape\n",
    "\n",
    "    if n == 0:\n",
    "        # Return a scalar tensor on the correct device\n",
    "        return torch.tensor(0, dtype=torch.long, device=output.device) # Use long for counts\n",
    "\n",
    "    # 0. Find the index of the maximum value in the output (common for both calculations)\n",
    "    output_argmax = torch.argmax(output, dim=1)\n",
    "    \n",
    "    # --- Calculation for the original target ---\n",
    "\n",
    "    # 1. Find the maximum value in the original target\n",
    "    target_max_values = torch.max(target, dim=1, keepdim=True)[0]\n",
    "\n",
    "    # 2. Create a boolean mask for the original target\n",
    "    target_max_mask = (target == target_max_values)\n",
    "\n",
    "    # 3. Check if the output_argmax index is True in the original target_max_mask\n",
    "    # Ensure arange is on the same device as the tensors\n",
    "    correct_predictions_bool_original = target_max_mask[torch.arange(n, device=output.device), output_argmax]\n",
    "\n",
    "    # 4. Sum the boolean tensor to get the count of correct predictions for original target.\n",
    "    correct_count_original = correct_predictions_bool_original.sum()\n",
    "\n",
    "    # --- Calculation for the clipped target ---\n",
    "\n",
    "    # 1. Clip the target values to (-1, 1)\n",
    "    target_clipped = torch.clip(target, -1, 1)\n",
    "\n",
    "    # 2. Find the maximum value in the clipped target\n",
    "    target_clipped_max_values = torch.max(target_clipped, dim=1, keepdim=True)[0]\n",
    "\n",
    "    # 3. Create a boolean mask for the clipped target\n",
    "    target_clipped_max_mask = (target_clipped == target_clipped_max_values)\n",
    "\n",
    "    # 4. Check if the output_argmax index is True in the clipped target_max_mask\n",
    "    correct_predictions_bool_clipped = target_clipped_max_mask[torch.arange(n, device=output.device), output_argmax]\n",
    "\n",
    "    # 5. Sum the boolean tensor to get the count of correct predictions for clipped target.\n",
    "    correct_count_clipped = correct_predictions_bool_clipped.sum()\n",
    "\n",
    "    return correct_count_original, correct_count_clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f791ba75",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:13.313332Z",
     "iopub.status.busy": "2025-07-09T12:33:13.312959Z",
     "iopub.status.idle": "2025-07-09T12:33:13.325850Z",
     "shell.execute_reply": "2025-07-09T12:33:13.324447Z"
    },
    "papermill": {
     "duration": 0.019772,
     "end_time": "2025-07-09T12:33:13.327606",
     "exception": false,
     "start_time": "2025-07-09T12:33:13.307834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting layers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile layers.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "\n",
    "class LayerNorm2d(nn.LayerNorm):\n",
    "    \"\"\" LayerNorm for channels of '2D' spatial NCHW tensors \"\"\"\n",
    "    def __init__(self, num_channels, eps=1e-6, affine=True):\n",
    "        super().__init__(num_channels, eps=eps, elementwise_affine=affine)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        reduced_channel = max(channel // reduction, 1)\n",
    "        self.fc_scale = nn.Sequential(\n",
    "            nn.Linear(channel, reduced_channel, bias=True),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Linear(reduced_channel, channel, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.fc_offset = nn.Sequential(\n",
    "            nn.Linear(channel, reduced_channel, bias=True),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Linear(reduced_channel, channel, bias=True),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y_scale = self.fc_scale(y).view(b, c, 1, 1)\n",
    "        y_offset = self.fc_offset(y).view(b, c, 1, 1)\n",
    "        return x * y_scale.expand_as(x) + y_offset.expand_as(x)\n",
    "\n",
    "class InitialExtractor(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super(InitialExtractor, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.BatchNorm2d(in_channels))\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2, stride=1, bias=False))\n",
    "        self.convs = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convs(x)\n",
    "        return out\n",
    "\n",
    "class Grouped1x1SumConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_groups):\n",
    "        super().__init__()\n",
    "        assert in_channels % num_groups == 0, \"in_channels must be divisible by num_groups\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_groups = num_groups\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels * num_groups,\n",
    "            kernel_size=1,\n",
    "            groups=num_groups,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, H, W = x.shape\n",
    "        G = self.num_groups\n",
    "        Cout = self.out_channels\n",
    "\n",
    "        out = self.conv(x)\n",
    "\n",
    "        out = out.view(B, G, Cout, H, W)\n",
    "        out = out.sum(dim=1)\n",
    "        return out\n",
    "\n",
    "class ConnectFourBlock(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size = (5, 5), mlp_factor=4, group_factor=2):\n",
    "        super().__init__()\n",
    "\n",
    "        ks_h, ks_w = kernel_size\n",
    "        hidden_dim = in_channels * mlp_factor\n",
    "        layers_0 = []\n",
    "\n",
    "        layers_0.append(nn.Conv2d(in_channels, in_channels, kernel_size=(ks_h, ks_w),  padding=(ks_h // 2, ks_w // 2), groups=in_channels, bias=False))\n",
    "        layers_0.append(LayerNorm2d(in_channels))\n",
    "\n",
    "        layers_0.append(nn.Conv2d(in_channels, hidden_dim, kernel_size=1, bias=False))\n",
    "        layers_0.append(nn.SiLU(inplace=True))\n",
    "\n",
    "        layers_0.append(nn.Conv2d(hidden_dim, in_channels, kernel_size=1, bias=False))\n",
    "\n",
    "        self.conv_next = nn.Sequential(*layers_0)\n",
    "\n",
    "        hidden_dim = in_channels * group_factor\n",
    "        n_groups = 2 * group_factor\n",
    "        ks_h, ks_w = 3, 3\n",
    "        \n",
    "        layers_1 = []\n",
    "        layers_1.append(LayerNorm2d(in_channels))\n",
    "        layers_1.append(nn.Conv2d(in_channels, hidden_dim, kernel_size=1, bias=False))\n",
    "        self.proj_up = nn.Sequential(*layers_1)\n",
    "\n",
    "        layers_2 = []\n",
    "        layers_2.append(nn.Conv2d(hidden_dim, hidden_dim, kernel_size=(ks_h, ks_w), padding=(ks_h // 2, ks_w // 2), groups=n_groups, bias=False))\n",
    "        layers_2.append(Grouped1x1SumConv(hidden_dim, hidden_dim, num_groups=n_groups))\n",
    "        layers_2.append(nn.SiLU(inplace=True))\n",
    "        self.grouped_0 = nn.Sequential(*layers_2)\n",
    "\n",
    "        layers_3 = []\n",
    "        layers_3.append(nn.Conv2d(hidden_dim, hidden_dim, kernel_size=(ks_h, ks_w), padding=(ks_h // 2, ks_w // 2), groups=n_groups, bias=False))\n",
    "        layers_3.append(Grouped1x1SumConv(hidden_dim, hidden_dim, num_groups=n_groups))\n",
    "        layers_3.append(nn.SiLU(inplace=True))\n",
    "        self.grouped_1 = nn.Sequential(*layers_3)\n",
    "\n",
    "        layers_4 = []\n",
    "        layers_4.append(SEBlock(hidden_dim))\n",
    "        layers_4.append(nn.Conv2d(hidden_dim, in_channels, kernel_size=1, bias=False))\n",
    "        self.proj_down = nn.Sequential(*layers_4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        summand = self.conv_next(x)\n",
    "        out = out + summand\n",
    "\n",
    "        up_proj = self.proj_up(out)\n",
    "        summand = self.grouped_0(up_proj)\n",
    "        up_proj = up_proj + summand\n",
    "        summand = self.grouped_1(up_proj)\n",
    "        up_proj = up_proj + summand\n",
    "\n",
    "        summand = self.proj_down(up_proj)\n",
    "        out = out + summand\n",
    "        \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a4b8f7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:13.336788Z",
     "iopub.status.busy": "2025-07-09T12:33:13.336481Z",
     "iopub.status.idle": "2025-07-09T12:33:13.344303Z",
     "shell.execute_reply": "2025-07-09T12:33:13.342802Z"
    },
    "papermill": {
     "duration": 0.01485,
     "end_time": "2025-07-09T12:33:13.346463",
     "exception": false,
     "start_time": "2025-07-09T12:33:13.331613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vit.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile vit.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    return nn.init.trunc_normal_(tensor, mean=mean, std=std, a=a, b=b)\n",
    "\n",
    "class VisionTransformerWithTasks(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape,\n",
    "        num_layers,\n",
    "        embed_dim=160,\n",
    "        num_heads=5,\n",
    "        mlp_dim=320,\n",
    "        num_tasks=7 + 7 + 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        C, H, W = input_shape\n",
    "        self.num_patches = H * W\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.proj = nn.Linear(C, embed_dim)\n",
    "        self.task_tokens = nn.Parameter(torch.randn(1, num_tasks, embed_dim))\n",
    "        self.pos_embed = nn.Embedding(self.num_patches + num_tasks, embed_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=mlp_dim,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\"\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.output_head = nn.Conv1d(\n",
    "            in_channels=embed_dim * num_tasks,\n",
    "            out_channels=num_tasks,\n",
    "            kernel_size=1,\n",
    "            groups=num_tasks\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        trunc_normal_(self.task_tokens, std=0.02)\n",
    "        trunc_normal_(self.pos_embed.weight, std=0.02)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Conv1d):\n",
    "                trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        task_tokens = self.task_tokens.expand(B, -1, -1)\n",
    "        x = torch.cat((task_tokens, x), dim=1)\n",
    "\n",
    "        pos_ids = torch.arange(x.size(1), device=x.device).unsqueeze(0).expand(B, -1)\n",
    "        x = x + self.pos_embed(pos_ids)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "        task_outputs = x[:, :self.num_tasks]\n",
    "\n",
    "        task_outputs = task_outputs.reshape(B, self.num_tasks * self.embed_dim, 1)\n",
    "        logits = self.output_head(task_outputs).squeeze(-1)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "195c59c3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:13.356077Z",
     "iopub.status.busy": "2025-07-09T12:33:13.355730Z",
     "iopub.status.idle": "2025-07-09T12:33:13.364522Z",
     "shell.execute_reply": "2025-07-09T12:33:13.363124Z"
    },
    "papermill": {
     "duration": 0.015723,
     "end_time": "2025-07-09T12:33:13.366351",
     "exception": false,
     "start_time": "2025-07-09T12:33:13.350628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting c4model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile c4model.py\n",
    "import layers\n",
    "from vit import VisionTransformerWithTasks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "input_channels = 2\n",
    "input_height = 6\n",
    "input_width = 7\n",
    "output_width = 7\n",
    "\n",
    "class Connect4Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Connect4Model, self).__init__()\n",
    "\n",
    "        extractor_layers = []\n",
    "        stage_0_width = 64\n",
    "        num_conv_blocks = 2\n",
    "        num_transformer_layers = 5\n",
    "        \n",
    "        extractor_layers.append(\n",
    "            layers.InitialExtractor(in_channels=input_channels, out_channels=stage_0_width),\n",
    "        )\n",
    "        \n",
    "        extractor_layers.extend([\n",
    "            layers.ConnectFourBlock(in_channels=stage_0_width,) for _ in range(num_conv_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.conv_extractor = nn.Sequential(*extractor_layers)\n",
    "\n",
    "        self.transformer = VisionTransformerWithTasks(\n",
    "            input_shape=(stage_0_width, input_height, input_width),\n",
    "            num_layers=num_transformer_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        convnet_features = self.conv_extractor(x)\n",
    "        concatenated_output = self.transformer(convnet_features)\n",
    "\n",
    "        policy_outputs, value_outputs, next_value_outputs \\\n",
    "            = torch.split(concatenated_output, [7, 1, 7], dim=-1)\n",
    "        return policy_outputs, value_outputs, next_value_outputs\n",
    "    \n",
    "def get_custom_model():\n",
    "    return Connect4Model()\n",
    "\n",
    "def count_model_flops(model=None):\n",
    "    from torch.utils.flop_counter import FlopCounterMode\n",
    "    if model is None:\n",
    "        model = get_custom_model()\n",
    "    print(\"Model Architecture:\")\n",
    "    print(model)\n",
    "    \n",
    "    dummy_input = torch.randn(1, input_channels, input_height, input_width)\n",
    "    \n",
    "    with FlopCounterMode(model) as count:\n",
    "        dummy_output = model(dummy_input)\n",
    "        total_flops = count.get_total_flops()\n",
    "    \n",
    "    print(f\"Total FLOPS: {total_flops}\")\n",
    "    print(\"Dummy Input Shape:\", dummy_input.shape)\n",
    "    print(\"Dummy Output:\", dummy_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f358edb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:13.375795Z",
     "iopub.status.busy": "2025-07-09T12:33:13.375489Z",
     "iopub.status.idle": "2025-07-09T12:33:13.383441Z",
     "shell.execute_reply": "2025-07-09T12:33:13.382531Z"
    },
    "papermill": {
     "duration": 0.014449,
     "end_time": "2025-07-09T12:33:13.384827",
     "exception": false,
     "start_time": "2025-07-09T12:33:13.370378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "from c4model import *\n",
    "from utils import *\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "def train_model(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    model: nn.Module,\n",
    "    checkpoint_path: str = \"checkpoint.pth\",\n",
    "    best_model_path: str = \"best_model.pth\",\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 0.001,\n",
    "    epochs: int = 10,\n",
    "    subsample_size: Optional[int] = None,\n",
    "    warmup_fraction: float = 0.1,\n",
    "    weight_decay: float = 0.01,\n",
    "):\n",
    "    \n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "    def move_to_device(batch):\n",
    "        obs, targets = batch\n",
    "        obs = obs.to(device, torch.float32)\n",
    "        targets = targets.to(device, torch.float32)\n",
    "        return obs, targets\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    subsample_size = min(subsample_size, len(train_dataset))\n",
    "    num_batches_per_epoch = math.floor(max(0, subsample_size) / batch_size) if batch_size > 0 else 0\n",
    "\n",
    "    total_steps = num_batches_per_epoch * epochs\n",
    "    \n",
    "    mse_loss = nn.MSELoss()\n",
    "    kl_criterion = nn.KLDivLoss(reduction='batchmean', log_target=True)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, math.ceil(total_steps * warmup_fraction), total_steps)\n",
    "        \n",
    "    start_epoch = 0\n",
    "    best_val_acc = 0\n",
    "    model.to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(f\"Begin epoch {epoch} with LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        num_samples_in_epoch = 0\n",
    "        loader_subset = itertools.islice(train_loader, num_batches_per_epoch)\n",
    "        for step, batch in enumerate(loader_subset):\n",
    "            inputs, labels = move_to_device(batch)\n",
    "            target_policy_log_probs = F.log_softmax(labels / 10, dim=-1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            policy_outputs, value_outputs, next_value_outputs = model(inputs)\n",
    "            policy_outputs = F.log_softmax(policy_outputs / 4, dim=-1)\n",
    "\n",
    "            kl_loss = 10000 * kl_criterion(policy_outputs, target_policy_log_probs)\n",
    "            value_loss = mse_loss(value_outputs, torch.max(labels, dim=-1).values.unsqueeze(1))\n",
    "            next_value_loss = 0.1 * mse_loss(next_value_outputs, labels)\n",
    "            \n",
    "            loss = kl_loss + value_loss + next_value_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            batch_train_loss = loss.item()\n",
    "            num_samples_in_epoch += inputs.size(0)\n",
    "            running_loss += batch_train_loss * inputs.size(0)\n",
    "            if (step + 1) % math.ceil(num_batches_per_epoch / 10) == 0:\n",
    "                running_loss_avg = running_loss / num_samples_in_epoch\n",
    "                print(f\"Epoch [{epoch}/{epochs}], Step [{step} / {num_batches_per_epoch}], Running Loss: {running_loss_avg} Batch Loss: {kl_loss.item():.4f} + {value_loss.item():.4f} + {next_value_loss.item():.4f} = {batch_train_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        num_samples_in_val = 0\n",
    "        val_loss = 0.0\n",
    "        val_correct_count_strong = val_correct_count_weak = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in itertools.islice(val_loader, num_batches_per_epoch):\n",
    "                inputs, labels = move_to_device(batch)\n",
    "                target_policy_log_probs = F.log_softmax(labels / 10, dim=-1)\n",
    "                \n",
    "                policy_outputs, value_outputs, next_value_outputs = model(inputs)\n",
    "                policy_outputs = F.log_softmax(policy_outputs / 4, dim=-1)\n",
    "    \n",
    "                kl_loss = 10000 * kl_criterion(policy_outputs, target_policy_log_probs)\n",
    "                value_loss = mse_loss(value_outputs, torch.max(labels, dim=-1).values.unsqueeze(1))\n",
    "                next_value_loss = 0.1 * mse_loss(next_value_outputs, labels)\n",
    "                    \n",
    "                loss = kl_loss + value_loss + next_value_loss\n",
    "                \n",
    "                batch_correct_count_strong, batch_correct_count_weak = argmax_accuracy(policy_outputs, labels)\n",
    "                val_correct_count_strong += batch_correct_count_strong\n",
    "                val_correct_count_weak += batch_correct_count_weak\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                num_samples_in_val += inputs.size(0)\n",
    "\n",
    "        epoch_train_loss = running_loss / num_samples_in_epoch\n",
    "        epoch_val_loss = val_loss / num_samples_in_val\n",
    "        epoch_val_policy_acc = {\"strong\": val_correct_count_strong / num_samples_in_val, \"weak\": val_correct_count_weak / num_samples_in_val}\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{epochs}], Training Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}, Validation Policy Acc: {epoch_val_policy_acc}\")\n",
    "        \n",
    "        # Save checkpoint and best model based on validation accuracy\n",
    "        if epoch_val_policy_acc[\"strong\"] > best_val_acc:\n",
    "            best_val_acc = epoch_val_policy_acc[\"strong\"]\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Validation loss improved. Saved best model to {best_model_path}\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    print(f\"Best validation acc achieved: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcab5665",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:13.394150Z",
     "iopub.status.busy": "2025-07-09T12:33:13.393823Z",
     "iopub.status.idle": "2025-07-09T12:33:22.287478Z",
     "shell.execute_reply": "2025-07-09T12:33:22.286306Z"
    },
    "papermill": {
     "duration": 8.900562,
     "end_time": "2025-07-09T12:33:22.289397",
     "exception": false,
     "start_time": "2025-07-09T12:33:13.388835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/timm/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import train\n",
    "import c4model\n",
    "import layers\n",
    "import os\n",
    "\n",
    "import sys\n",
    "del sys.modules[\"layers\"]\n",
    "del sys.modules[\"c4model\"]\n",
    "del sys.modules[\"train\"]\n",
    "import c4model\n",
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b63742f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:22.860377Z",
     "iopub.status.busy": "2025-07-09T12:33:22.859952Z",
     "iopub.status.idle": "2025-07-09T12:36:28.166052Z",
     "shell.execute_reply": "2025-07-09T12:36:28.165125Z"
    },
    "papermill": {
     "duration": 185.313175,
     "end_time": "2025-07-09T12:36:28.167802",
     "exception": false,
     "start_time": "2025-07-09T12:33:22.854627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path):\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from torch.utils.data import TensorDataset\n",
    "    import gc\n",
    "    import torch\n",
    "\n",
    "    def adjust_targets(targets_np):\n",
    "        mask_pos = targets_np > 0\n",
    "        targets_np[mask_pos] += 100\n",
    "        mask_neg = targets_np < 0\n",
    "        targets_np[mask_neg] -= 100\n",
    "\n",
    "    try:\n",
    "        print(\"Loading dataset with mmap_mode='r'...\")\n",
    "\n",
    "        dataset = np.load(dataset_path, mmap_mode='r')\n",
    "        full_train_obs = dataset[\"x_train\"]\n",
    "        full_train_targets = dataset[\"y_train\"]\n",
    "\n",
    "        print(\"Creating train/validation splits...\")\n",
    "        train_obs_np, val_obs_np, train_targets_np, val_targets_np = train_test_split(\n",
    "            full_train_obs, full_train_targets, test_size=0.1, random_state=12505\n",
    "        )\n",
    "\n",
    "        adjust_targets(train_targets_np)\n",
    "        adjust_targets(val_targets_np)\n",
    "\n",
    "        del full_train_obs\n",
    "        del full_train_targets\n",
    "        del dataset\n",
    "        gc.collect() # Force garbage collection\n",
    "\n",
    "        print(\"Converting NumPy arrays to PyTorch tensors with float16 precision (1/4)...\")\n",
    "        train_obs = torch.from_numpy(train_obs_np).to(dtype=torch.float16)\n",
    "        del train_obs_np\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Converting NumPy arrays to PyTorch tensors with float16 precision (2/4)...\")\n",
    "        val_obs = torch.from_numpy(val_obs_np).to(dtype=torch.float16)\n",
    "        del val_obs_np\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Converting NumPy arrays to PyTorch tensors with float16 precision (3/4)...\")\n",
    "        train_targets = torch.from_numpy(train_targets_np).to(dtype=torch.float16)\n",
    "        del train_targets_np\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Converting NumPy arrays to PyTorch tensors with float16 precision (4/4)...\")\n",
    "        val_targets = torch.from_numpy(val_targets_np).to(dtype=torch.float16)\n",
    "        del val_targets_np\n",
    "        gc.collect()\n",
    "\n",
    "        # Create TensorDatasets for use with PyTorch DataLoaders\n",
    "        train_dataset = TensorDataset(train_obs, train_targets)\n",
    "        val_dataset = TensorDataset(val_obs, val_targets)\n",
    "\n",
    "        print(\"Dataset preparation complete.\")\n",
    "        return train_dataset, val_dataset\n",
    "    except Exception as e:\n",
    "        print(f\"could not load data: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e9e13ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:36:28.179713Z",
     "iopub.status.busy": "2025-07-09T12:36:28.179383Z",
     "iopub.status.idle": "2025-07-09T12:36:28.186447Z",
     "shell.execute_reply": "2025-07-09T12:36:28.185432Z"
    },
    "papermill": {
     "duration": 0.014946,
     "end_time": "2025-07-09T12:36:28.188311",
     "exception": false,
     "start_time": "2025-07-09T12:36:28.173365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_weights_only(filepath):\n",
    "    \"\"\"\n",
    "    Loads only the weights (state_dict) from a .pth file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the .pth file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The state dictionary containing the model weights, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the entire checkpoint\n",
    "        checkpoint = torch.load(filepath)\n",
    "\n",
    "        # Check if the loaded object is a state_dict directly\n",
    "        if isinstance(checkpoint, dict) and all(isinstance(k, str) for k in checkpoint.keys()):\n",
    "            # Assume it's a state_dict if all keys are strings (common for weights)\n",
    "            print(f\"Successfully loaded state_dict from {filepath}\")\n",
    "            return checkpoint\n",
    "        elif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n",
    "            # If it's a dictionary containing a 'state_dict' key (common for full checkpoints)\n",
    "            print(f\"Successfully loaded 'state_dict' from checkpoint in {filepath}\")\n",
    "            return checkpoint['state_dict']\n",
    "        else:\n",
    "            print(f\"Warning: The .pth file at {filepath} does not seem to contain a standard state_dict or a checkpoint with 'state_dict'.\")\n",
    "            print(\"Attempting to return the loaded object directly. Please inspect its content.\")\n",
    "            return checkpoint # Return whatever was loaded, might be a raw tensor or other data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "428fc888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:\n",
      "Connect4Model(\n",
      "  (conv_extractor): Sequential(\n",
      "    (0): InitialExtractor(\n",
      "      (convs): Sequential(\n",
      "        (0): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): ConnectFourBlock(\n",
      "      (conv_next): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "        (1): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): SiLU(inplace=True)\n",
      "        (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (proj_up): Sequential(\n",
      "        (0): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (grouped_0): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)\n",
      "        (1): Grouped1x1SumConv(\n",
      "          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), groups=4)\n",
      "        )\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "      (grouped_1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)\n",
      "        (1): Grouped1x1SumConv(\n",
      "          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), groups=4)\n",
      "        )\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "      (proj_down): Sequential(\n",
      "        (0): SEBlock(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc_scale): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "            (1): SiLU(inplace=True)\n",
      "            (2): Linear(in_features=32, out_features=128, bias=True)\n",
      "            (3): Sigmoid()\n",
      "          )\n",
      "          (fc_offset): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "            (1): SiLU(inplace=True)\n",
      "            (2): Linear(in_features=32, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (2): ConnectFourBlock(\n",
      "      (conv_next): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=64, bias=False)\n",
      "        (1): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): SiLU(inplace=True)\n",
      "        (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (proj_up): Sequential(\n",
      "        (0): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (grouped_0): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)\n",
      "        (1): Grouped1x1SumConv(\n",
      "          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), groups=4)\n",
      "        )\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "      (grouped_1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)\n",
      "        (1): Grouped1x1SumConv(\n",
      "          (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), groups=4)\n",
      "        )\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "      (proj_down): Sequential(\n",
      "        (0): SEBlock(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc_scale): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "            (1): SiLU(inplace=True)\n",
      "            (2): Linear(in_features=32, out_features=128, bias=True)\n",
      "            (3): Sigmoid()\n",
      "          )\n",
      "          (fc_offset): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "            (1): SiLU(inplace=True)\n",
      "            (2): Linear(in_features=32, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer): VisionTransformerWithTasks(\n",
      "    (proj): Linear(in_features=64, out_features=160, bias=True)\n",
      "    (pos_embed): Embedding(57, 160)\n",
      "    (transformer): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-4): 5 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=160, out_features=160, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=160, out_features=320, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=320, out_features=160, bias=True)\n",
      "          (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output_head): Conv1d(2400, 15, kernel_size=(1,), stride=(1,), groups=15)\n",
      "  )\n",
      ")\n",
      "Module                             FLOP    % Total\n",
      "-----------------------------  --------  ---------\n",
      "Connect4Model                  154.578M    100.00%\n",
      " - aten.convolution             26.519M     17.16%\n",
      " - aten.addmm                  116.802M     75.56%\n",
      " - aten.mm                       0.860M      0.56%\n",
      " - aten.bmm                     10.397M      6.73%\n",
      " Connect4Model.conv_extractor   26.580M     17.20%\n",
      "  - aten.convolution            26.514M     17.15%\n",
      "  - aten.addmm                   0.066M      0.04%\n",
      " Connect4Model.transformer     127.998M     82.80%\n",
      "  - aten.mm                      0.860M      0.56%\n",
      "  - aten.addmm                 116.736M     75.52%\n",
      "  - aten.bmm                    10.397M      6.73%\n",
      "  - aten.convolution             0.005M      0.00%\n",
      "Total FLOPS: 154577728\n",
      "Dummy Input Shape: torch.Size([1, 2, 6, 7])\n",
      "Dummy Output: (tensor([[-0.0944,  0.0150, -0.1295,  0.1782, -0.1629,  0.3332, -0.2351]],\n",
      "       grad_fn=<SplitWithSizesBackward0>), tensor([[-0.0960]], grad_fn=<SplitWithSizesBackward0>), tensor([[ 0.1705, -0.1668,  0.0179, -0.2896, -0.0220,  0.0879, -0.1340]],\n",
      "       grad_fn=<SplitWithSizesBackward0>))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/timm/lib/python3.13/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n",
      "/Users/spm00004/Downloads/c4project/c4model.py:61: UserWarning: mods argument is not needed anymore, you can stop passing it\n",
      "  with FlopCounterMode(model) as count:\n"
     ]
    }
   ],
   "source": [
    "model = c4model.get_custom_model()\n",
    "c4model.count_model_flops(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de4248d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset with mmap_mode='r'...\n",
      "Creating train/validation splits...\n",
      "Converting NumPy arrays to PyTorch tensors with float16 precision (1/4)...\n",
      "Converting NumPy arrays to PyTorch tensors with float16 precision (2/4)...\n",
      "Converting NumPy arrays to PyTorch tensors with float16 precision (3/4)...\n",
      "Converting NumPy arrays to PyTorch tensors with float16 precision (4/4)...\n",
      "Dataset preparation complete.\n"
     ]
    }
   ],
   "source": [
    "dataset_splits = load_dataset(\"./c4_data.npz\")\n",
    "if dataset_splits is None:\n",
    "    os._exit(1)\n",
    "\n",
    "train_dataset, val_dataset = dataset_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a34d8adc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:36:28.199147Z",
     "iopub.status.busy": "2025-07-09T12:36:28.198693Z",
     "iopub.status.idle": "2025-07-09T12:36:34.229830Z",
     "shell.execute_reply": "2025-07-09T12:36:34.228550Z"
    },
    "papermill": {
     "duration": 6.038598,
     "end_time": "2025-07-09T12:36:34.231422",
     "exception": true,
     "start_time": "2025-07-09T12:36:28.192824",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File not found at v1_0_starting_checkpoint.pth\n",
      "Begin epoch 0 with LR: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/timm/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/12], Step [1638 / 16384], Running Loss: 21076.174521669083 Batch Loss: 4800.4590 + 8690.2803 + 5717.6406 = 19208.3789, LR: 0.000042\n",
      "Epoch [0/12], Step [3277 / 16384], Running Loss: 18928.92789107783 Batch Loss: 3331.3420 + 6303.9111 + 5063.1714 = 14698.4238, LR: 0.000083\n",
      "Epoch [0/12], Step [4916 / 16384], Running Loss: 16838.458346044335 Batch Loss: 2760.4419 + 3197.9858 + 4498.6162 = 10457.0439, LR: 0.000125\n",
      "Epoch [0/12], Step [6555 / 16384], Running Loss: 15010.384118332086 Batch Loss: 2618.9136 + 1916.7490 + 4140.8403 = 8676.5029, LR: 0.000167\n",
      "Epoch [0/12], Step [8194 / 16384], Running Loss: 13606.58859340442 Batch Loss: 2541.7820 + 1915.8936 + 3340.4583 = 7798.1338, LR: 0.000208\n",
      "Epoch [0/12], Step [9833 / 16384], Running Loss: 12872.303769477625 Batch Loss: 2832.5063 + 2080.2229 + 2361.7163 = 7274.4458, LR: 0.000250\n",
      "Epoch [0/12], Step [11472 / 16384], Running Loss: 11930.67132232674 Batch Loss: 2307.3345 + 1817.3192 + 1131.4760 = 5256.1299, LR: 0.000292\n",
      "Epoch [0/12], Step [13111 / 16384], Running Loss: 11019.488928915307 Batch Loss: 2023.7130 + 1536.8782 + 432.3752 = 3992.9666, LR: 0.000333\n",
      "Epoch [0/12], Step [14750 / 16384], Running Loss: 10234.594863669201 Batch Loss: 2089.3625 + 1652.1996 + 256.1824 = 3997.7444, LR: 0.000375\n",
      "Epoch [0/12], Training Loss: 9862.2972, Validation Loss: 4667.5842, Validation Policy Acc: {'strong': tensor(0.7885, device='mps:0'), 'weak': tensor(0.9537, device='mps:0')}\n",
      "Validation loss improved. Saved best model to v1_0_best_model.pth\n",
      "Begin epoch 1 with LR: 0.000417\n",
      "Epoch [1/12], Step [1638 / 16384], Running Loss: 4110.334838196881 Batch Loss: 1913.5626 + 1121.9019 + 175.6573 = 3211.1216, LR: 0.000458\n",
      "Epoch [1/12], Step [3277 / 16384], Running Loss: 3701.3131584646935 Batch Loss: 1710.2617 + 1181.4000 + 158.6412 = 3050.3027, LR: 0.000500\n",
      "Epoch [1/12], Step [4916 / 16384], Running Loss: 3457.654817330975 Batch Loss: 1717.4915 + 1403.7197 + 163.8617 = 3285.0730, LR: 0.000542\n",
      "Epoch [1/12], Step [6555 / 16384], Running Loss: 3286.0105939379837 Batch Loss: 1612.0859 + 982.4937 + 142.6529 = 2737.2324, LR: 0.000583\n",
      "Epoch [1/12], Step [8194 / 16384], Running Loss: 3150.0823602961505 Batch Loss: 1517.5361 + 975.4055 + 136.4291 = 2629.3708, LR: 0.000625\n",
      "Epoch [1/12], Step [9833 / 16384], Running Loss: 3044.9735983041137 Batch Loss: 1390.8549 + 962.4181 + 125.9831 = 2479.2561, LR: 0.000667\n",
      "Epoch [1/12], Step [11472 / 16384], Running Loss: 2951.6122613622747 Batch Loss: 1345.5316 + 832.4812 + 117.1804 = 2295.1931, LR: 0.000708\n",
      "Epoch [1/12], Step [13111 / 16384], Running Loss: 2876.360123993348 Batch Loss: 1330.2286 + 849.9629 + 125.2739 = 2305.4653, LR: 0.000750\n",
      "Epoch [1/12], Step [14750 / 16384], Running Loss: 2816.4017013809444 Batch Loss: 1196.8561 + 727.2527 + 121.1951 = 2045.3038, LR: 0.000792\n",
      "Epoch [1/12], Training Loss: 2758.8205, Validation Loss: 2019.7520, Validation Policy Acc: {'strong': tensor(0.8704, device='mps:0'), 'weak': tensor(0.9849, device='mps:0')}\n",
      "Validation loss improved. Saved best model to v1_0_best_model.pth\n",
      "Begin epoch 2 with LR: 0.000833\n",
      "Epoch [2/12], Step [1638 / 16384], Running Loss: 2197.529881903862 Batch Loss: 1210.0525 + 799.4874 + 113.2655 = 2122.8054, LR: 0.000875\n",
      "Epoch [2/12], Step [3277 / 16384], Running Loss: 2184.5292450751817 Batch Loss: 1601.1786 + 1463.0869 + 152.8897 = 3217.1553, LR: 0.000917\n",
      "Epoch [2/12], Step [4916 / 16384], Running Loss: 2169.5514771773946 Batch Loss: 1132.8257 + 876.8954 + 105.4325 = 2115.1536, LR: 0.000958\n",
      "Epoch [2/12], Step [6555 / 16384], Running Loss: 2157.6114253567225 Batch Loss: 1198.2174 + 728.3616 + 112.1520 = 2038.7310, LR: 0.001000\n",
      "Epoch [2/12], Step [8194 / 16384], Running Loss: 2144.0383505299296 Batch Loss: 1325.5526 + 707.5195 + 104.6315 = 2137.7036, LR: 0.001000\n",
      "Epoch [2/12], Step [9833 / 16384], Running Loss: 2128.7553739239343 Batch Loss: 1100.7659 + 773.1456 + 103.9160 = 1977.8274, LR: 0.000999\n",
      "Epoch [2/12], Step [11472 / 16384], Running Loss: 2113.0025084050085 Batch Loss: 1242.7356 + 944.7609 + 108.7083 = 2296.2048, LR: 0.000998\n",
      "Epoch [2/12], Step [13111 / 16384], Running Loss: 2096.4406661568364 Batch Loss: 1170.5706 + 726.4653 + 94.7013 = 1991.7372, LR: 0.000996\n",
      "Epoch [2/12], Step [14750 / 16384], Running Loss: 2080.735797473741 Batch Loss: 1238.9183 + 766.3180 + 105.2597 = 2110.4961, LR: 0.000993\n",
      "Epoch [2/12], Training Loss: 2067.0806, Validation Loss: 1789.5184, Validation Policy Acc: {'strong': tensor(0.9020, device='mps:0'), 'weak': tensor(0.9882, device='mps:0')}\n",
      "Validation loss improved. Saved best model to v1_0_best_model.pth\n",
      "Begin epoch 3 with LR: 0.000990\n",
      "Epoch [3/12], Step [1638 / 16384], Running Loss: 1927.8555568581023 Batch Loss: 1157.3430 + 662.1846 + 92.6143 = 1912.1418, LR: 0.000987\n",
      "Epoch [3/12], Step [3277 / 16384], Running Loss: 1896.046775831836 Batch Loss: 1086.6443 + 706.2266 + 101.5874 = 1894.4583, LR: 0.000983\n",
      "Epoch [3/12], Step [4916 / 16384], Running Loss: 1892.011298889694 Batch Loss: 1058.0638 + 582.8157 + 84.2842 = 1725.1637, LR: 0.000978\n",
      "Epoch [3/12], Step [6555 / 16384], Running Loss: 1884.1456652414952 Batch Loss: 1152.8650 + 639.9158 + 89.2604 = 1882.0411, LR: 0.000973\n",
      "Epoch [3/12], Step [8194 / 16384], Running Loss: 1873.8156699403457 Batch Loss: 1026.7783 + 732.0521 + 90.7726 = 1849.6029, LR: 0.000968\n",
      "Epoch [3/12], Step [9833 / 16384], Running Loss: 1865.4946243009942 Batch Loss: 1010.0658 + 702.5934 + 93.0875 = 1805.7466, LR: 0.000962\n",
      "Epoch [3/12], Step [11472 / 16384], Running Loss: 1856.6092318735427 Batch Loss: 961.9391 + 428.7232 + 82.0709 = 1472.7333, LR: 0.000955\n",
      "Epoch [3/12], Step [13111 / 16384], Running Loss: 1850.0240635479129 Batch Loss: 1086.5411 + 555.0818 + 82.2240 = 1723.8470, LR: 0.000948\n",
      "Epoch [3/12], Step [14750 / 16384], Running Loss: 1842.949015613944 Batch Loss: 1007.1323 + 658.7036 + 84.7296 = 1750.5656, LR: 0.000941\n",
      "Epoch [3/12], Training Loss: 1836.8388, Validation Loss: 1717.7582, Validation Policy Acc: {'strong': tensor(0.9036, device='mps:0'), 'weak': tensor(0.9896, device='mps:0')}\n",
      "Validation loss improved. Saved best model to v1_0_best_model.pth\n",
      "Begin epoch 4 with LR: 0.000933\n",
      "Epoch [4/12], Step [1638 / 16384], Running Loss: 1733.7816838374438 Batch Loss: 993.4203 + 697.7899 + 92.1413 = 1783.3516, LR: 0.000925\n",
      "Epoch [4/12], Step [3277 / 16384], Running Loss: 1744.0571918406088 Batch Loss: 1011.2578 + 679.1810 + 83.6619 = 1774.1006, LR: 0.000916\n",
      "Epoch [4/12], Step [4916 / 16384], Running Loss: 1753.5749955114272 Batch Loss: 990.3798 + 539.8915 + 86.1353 = 1616.4065, LR: 0.000906\n",
      "Epoch [4/12], Step [6555 / 16384], Running Loss: 1747.6485160003717 Batch Loss: 1045.1823 + 601.1877 + 80.5381 = 1726.9080, LR: 0.000897\n",
      "Epoch [4/12], Step [8194 / 16384], Running Loss: 1737.9135217560727 Batch Loss: 989.7098 + 563.9738 + 79.4565 = 1633.1401, LR: 0.000886\n",
      "Epoch [4/12], Step [9833 / 16384], Running Loss: 1731.8072579651314 Batch Loss: 964.0070 + 602.1680 + 74.9366 = 1641.1116, LR: 0.000876\n",
      "Epoch [4/12], Step [11472 / 16384], Running Loss: 1723.5653016269857 Batch Loss: 891.1614 + 496.9570 + 78.8131 = 1466.9315, LR: 0.000865\n",
      "Epoch [4/12], Step [13111 / 16384], Running Loss: 1718.6362930027867 Batch Loss: 1044.6425 + 648.6987 + 88.7091 = 1782.0503, LR: 0.000854\n",
      "Epoch [4/12], Step [14750 / 16384], Running Loss: 1711.668279797172 Batch Loss: 988.6086 + 570.5238 + 81.7789 = 1640.9113, LR: 0.000842\n",
      "Epoch [4/12], Training Loss: 1704.7615, Validation Loss: 1435.4256, Validation Policy Acc: {'strong': tensor(0.9132, device='mps:0'), 'weak': tensor(0.9907, device='mps:0')}\n",
      "Validation loss improved. Saved best model to v1_0_best_model.pth\n",
      "Begin epoch 5 with LR: 0.000830\n",
      "Epoch [5/12], Step [1638 / 16384], Running Loss: 1633.333015260353 Batch Loss: 900.6004 + 461.6400 + 74.7165 = 1436.9570, LR: 0.000817\n",
      "Epoch [5/12], Step [3277 / 16384], Running Loss: 1630.0173747613708 Batch Loss: 863.9023 + 423.0060 + 75.6903 = 1362.5985, LR: 0.000804\n",
      "Epoch [5/12], Step [4916 / 16384], Running Loss: 1617.5640996747175 Batch Loss: 1038.2107 + 587.2188 + 80.3347 = 1705.7642, LR: 0.000791\n",
      "Epoch [5/12], Step [6555 / 16384], Running Loss: 1609.3112641032546 Batch Loss: 1010.9398 + 621.5945 + 77.4180 = 1709.9524, LR: 0.000778\n",
      "Epoch [5/12], Step [8194 / 16384], Running Loss: 1601.2622939476958 Batch Loss: 881.7498 + 694.8349 + 74.1755 = 1650.7603, LR: 0.000764\n",
      "Epoch [5/12], Step [9833 / 16384], Running Loss: 1593.605915509469 Batch Loss: 823.0956 + 425.7982 + 72.7748 = 1321.6686, LR: 0.000750\n",
      "Epoch [5/12], Step [11472 / 16384], Running Loss: 1586.0751651593348 Batch Loss: 950.0809 + 546.3510 + 78.8480 = 1575.2799, LR: 0.000736\n",
      "Epoch [5/12], Step [13111 / 16384], Running Loss: 1580.364516243693 Batch Loss: 856.2330 + 457.2309 + 71.2922 = 1384.7560, LR: 0.000721\n",
      "Epoch [5/12], Step [14750 / 16384], Running Loss: 1574.7120437296226 Batch Loss: 941.6699 + 702.1437 + 73.5113 = 1717.3250, LR: 0.000706\n",
      "Epoch [5/12], Training Loss: 1567.3441, Validation Loss: 1452.0465, Validation Policy Acc: {'strong': tensor(0.9141, device='mps:0'), 'weak': tensor(0.9908, device='mps:0')}\n",
      "Validation loss improved. Saved best model to v1_0_best_model.pth\n",
      "Begin epoch 6 with LR: 0.000691\n",
      "Epoch [6/12], Step [1638 / 16384], Running Loss: 1484.0017525543158 Batch Loss: 773.4372 + 408.8594 + 65.3667 = 1247.6633, LR: 0.000676\n",
      "Epoch [6/12], Step [3277 / 16384], Running Loss: 1486.5628256946165 Batch Loss: 908.4070 + 511.5479 + 78.4231 = 1498.3779, LR: 0.000661\n",
      "Epoch [6/12], Step [4916 / 16384], Running Loss: 1477.8260181264857 Batch Loss: 873.7315 + 492.4848 + 70.7999 = 1437.0162, LR: 0.000645\n",
      "Epoch [6/12], Step [6555 / 16384], Running Loss: 1471.7931043009266 Batch Loss: 873.2902 + 446.0901 + 68.8352 = 1388.2156, LR: 0.000629\n",
      "Epoch [6/12], Step [8194 / 16384], Running Loss: 1468.7089379599795 Batch Loss: 862.6130 + 463.3891 + 69.4141 = 1395.4161, LR: 0.000614\n",
      "Epoch [6/12], Step [9833 / 16384], Running Loss: 1464.234090057553 Batch Loss: 868.2809 + 494.8194 + 77.0749 = 1440.1750, LR: 0.000598\n",
      "Epoch [6/12], Step [11472 / 16384], Running Loss: 1455.997859380618 Batch Loss: 869.1375 + 454.7269 + 63.5659 = 1387.4303, LR: 0.000581\n",
      "Epoch [6/12], Step [13111 / 16384], Running Loss: 1450.4348461809793 Batch Loss: 886.8551 + 437.2114 + 72.0084 = 1396.0750, LR: 0.000565\n",
      "Epoch [6/12], Step [14750 / 16384], Running Loss: 1443.773641124304 Batch Loss: 715.3686 + 386.8090 + 66.6939 = 1168.8715, LR: 0.000549\n",
      "Epoch [6/12], Training Loss: 1439.8867, Validation Loss: 1318.6144, Validation Policy Acc: {'strong': tensor(0.9195, device='mps:0'), 'weak': tensor(0.9920, device='mps:0')}\n",
      "Validation loss improved. Saved best model to v1_0_best_model.pth\n",
      "Begin epoch 7 with LR: 0.000533\n",
      "Epoch [7/12], Step [1638 / 16384], Running Loss: 1360.7116631070894 Batch Loss: 830.7642 + 449.1098 + 64.7039 = 1344.5779, LR: 0.000516\n",
      "Epoch [7/12], Step [3277 / 16384], Running Loss: 1360.0420499269812 Batch Loss: 790.9183 + 448.9380 + 63.2075 = 1303.0638, LR: 0.000500\n",
      "Epoch [7/12], Step [4916 / 16384], Running Loss: 1362.7329576363136 Batch Loss: 795.8596 + 330.4598 + 61.9255 = 1188.2449, LR: 0.000484\n",
      "Epoch [7/12], Step [6555 / 16384], Running Loss: 1354.324100803936 Batch Loss: 723.4948 + 385.8071 + 63.5445 = 1172.8464, LR: 0.000467\n",
      "Epoch [7/12], Step [8194 / 16384], Running Loss: 1347.7398751278051 Batch Loss: 901.3907 + 591.0016 + 71.7176 = 1564.1099, LR: 0.000451\n",
      "Epoch [7/12], Step [9833 / 16384], Running Loss: 1342.5737403806013 Batch Loss: 755.9156 + 456.5934 + 61.4802 = 1273.9891, LR: 0.000435\n",
      "Epoch [7/12], Step [11472 / 16384], Running Loss: 1337.0560904999697 Batch Loss: 790.5515 + 444.8860 + 67.3650 = 1302.8025, LR: 0.000419\n",
      "Epoch [7/12], Step [13111 / 16384], Running Loss: 1332.0985938928127 Batch Loss: 760.4958 + 466.5298 + 65.7268 = 1292.7524, LR: 0.000402\n",
      "Epoch [7/12], Step [14750 / 16384], Running Loss: 1326.7117941189633 Batch Loss: 769.5588 + 485.9591 + 57.3136 = 1312.8314, LR: 0.000386\n",
      "Epoch [7/12], Training Loss: 1321.5228, Validation Loss: 1151.7859, Validation Policy Acc: {'strong': tensor(0.9241, device='mps:0'), 'weak': tensor(0.9930, device='mps:0')}\n",
      "Validation loss improved. Saved best model to v1_0_best_model.pth\n",
      "Begin epoch 8 with LR: 0.000371\n",
      "Epoch [8/12], Step [1638 / 16384], Running Loss: 1250.5355041392188 Batch Loss: 756.9736 + 473.3064 + 59.0274 = 1289.3074, LR: 0.000355\n",
      "Epoch [8/12], Step [3277 / 16384], Running Loss: 1250.436920501169 Batch Loss: 716.3262 + 350.3508 + 57.8016 = 1124.4786, LR: 0.000339\n",
      "Epoch [8/12], Step [4916 / 16384], Running Loss: 1244.6816868265155 Batch Loss: 705.7667 + 371.6378 + 55.4761 = 1132.8806, LR: 0.000324\n",
      "Epoch [8/12], Step [6555 / 16384], Running Loss: 1241.9008426631349 Batch Loss: 734.7286 + 370.0380 + 62.3207 = 1167.0873, LR: 0.000309\n",
      "Epoch [8/12], Step [8194 / 16384], Running Loss: 1235.2800607268152 Batch Loss: 682.1184 + 389.7567 + 61.0075 = 1132.8824, LR: 0.000294\n",
      "Epoch [8/12], Step [9833 / 16384], Running Loss: 1229.7784626844189 Batch Loss: 759.8785 + 369.4836 + 61.5386 = 1190.9006, LR: 0.000279\n",
      "Epoch [8/12], Step [11472 / 16384], Running Loss: 1225.6330813728173 Batch Loss: 766.0414 + 388.7054 + 61.1560 = 1215.9028, LR: 0.000264\n",
      "Epoch [8/12], Step [13111 / 16384], Running Loss: 1219.9122761184665 Batch Loss: 666.6299 + 400.5821 + 58.6008 = 1125.8129, LR: 0.000250\n",
      "Epoch [8/12], Step [14750 / 16384], Running Loss: 1214.869035995077 Batch Loss: 809.0770 + 386.3191 + 63.8215 = 1259.2177, LR: 0.000236\n",
      "Epoch [8/12], Training Loss: 1209.4267, Validation Loss: 1060.7867, Validation Policy Acc: {'strong': tensor(0.9275, device='mps:0'), 'weak': tensor(0.9936, device='mps:0')}\n",
      "Validation loss improved. Saved best model to v1_0_best_model.pth\n",
      "Begin epoch 9 with LR: 0.000222\n",
      "Epoch [9/12], Step [1638 / 16384], Running Loss: 1148.6518091058645 Batch Loss: 792.2001 + 368.4987 + 61.3664 = 1222.0651, LR: 0.000209\n",
      "Epoch [9/12], Step [3277 / 16384], Running Loss: 1145.365168312543 Batch Loss: 732.7446 + 475.2255 + 54.1699 = 1262.1399, LR: 0.000196\n",
      "Epoch [9/12], Step [4916 / 16384], Running Loss: 1138.8617182063067 Batch Loss: 647.1238 + 342.5062 + 53.6938 = 1043.3237, LR: 0.000183\n",
      "Epoch [9/12], Step [6555 / 16384], Running Loss: 1133.6391101811557 Batch Loss: 725.4343 + 402.7299 + 55.7473 = 1183.9114, LR: 0.000170\n",
      "Epoch [9/12], Step [8194 / 16384], Running Loss: 1129.9755250215094 Batch Loss: 641.6287 + 449.7137 + 56.2951 = 1147.6373, LR: 0.000158\n",
      "Epoch [9/12], Step [9833 / 16384], Running Loss: 1125.688927337603 Batch Loss: 638.5004 + 327.4542 + 51.2708 = 1017.2254, LR: 0.000146\n",
      "Epoch [9/12], Step [11472 / 16384], Running Loss: 1120.9412076210774 Batch Loss: 721.9639 + 372.8408 + 56.1776 = 1150.9823, LR: 0.000135\n",
      "Epoch [9/12], Step [13111 / 16384], Running Loss: 1115.9005305887797 Batch Loss: 676.7006 + 280.1655 + 50.9685 = 1007.8347, LR: 0.000124\n",
      "Epoch [9/12], Step [14750 / 16384], Running Loss: 1111.5473945043054 Batch Loss: 649.3328 + 339.7783 + 56.2476 = 1045.3586, LR: 0.000113\n",
      "Epoch [9/12], Training Loss: 1107.4507, Validation Loss: 977.1494, Validation Policy Acc: {'strong': tensor(0.9300, device='mps:0'), 'weak': tensor(0.9942, device='mps:0')}\n",
      "Validation loss improved. Saved best model to v1_0_best_model.pth\n",
      "Begin epoch 10 with LR: 0.000103\n",
      "Epoch [10/12], Step [1638 / 16384], Running Loss: 1057.4772054359198 Batch Loss: 680.1270 + 343.7988 + 50.4291 = 1074.3549, LR: 0.000094\n",
      "Epoch [10/12], Step [3277 / 16384], Running Loss: 1053.2650294138064 Batch Loss: 608.1249 + 312.0764 + 53.0707 = 973.2720, LR: 0.000084\n",
      "Epoch [10/12], Step [4916 / 16384], Running Loss: 1048.7552900859343 Batch Loss: 634.7919 + 397.7421 + 49.8510 = 1082.3849, LR: 0.000075\n",
      "Epoch [10/12], Step [6555 / 16384], Running Loss: 1045.6831701999033 Batch Loss: 596.0356 + 280.5234 + 52.9981 = 929.5570, LR: 0.000067\n",
      "Epoch [10/12], Step [8194 / 16384], Running Loss: 1042.320937218471 Batch Loss: 731.8804 + 462.7372 + 50.4229 = 1245.0404, LR: 0.000059\n",
      "Epoch [10/12], Step [9833 / 16384], Running Loss: 1038.832490279807 Batch Loss: 622.8645 + 359.3621 + 52.1746 = 1034.4011, LR: 0.000052\n",
      "Epoch [10/12], Step [11472 / 16384], Running Loss: 1035.6294799900445 Batch Loss: 669.6123 + 333.0136 + 54.7913 = 1057.4172, LR: 0.000045\n",
      "Epoch [10/12], Step [13111 / 16384], Running Loss: 1032.6619573410435 Batch Loss: 695.3273 + 413.3816 + 57.2286 = 1165.9375, LR: 0.000038\n",
      "Epoch [10/12], Step [14750 / 16384], Running Loss: 1029.1926213341335 Batch Loss: 633.8865 + 286.5461 + 54.1517 = 974.5843, LR: 0.000032\n",
      "Epoch [10/12], Training Loss: 1025.9198, Validation Loss: 914.1448, Validation Policy Acc: {'strong': tensor(0.9318, device='mps:0'), 'weak': tensor(0.9945, device='mps:0')}\n",
      "Validation loss improved. Saved best model to v1_0_best_model.pth\n",
      "Begin epoch 11 with LR: 0.000027\n",
      "Epoch [11/12], Step [1638 / 16384], Running Loss: 992.7594122243698 Batch Loss: 632.5091 + 276.7000 + 50.3283 = 959.5374, LR: 0.000022\n",
      "Epoch [11/12], Step [3277 / 16384], Running Loss: 988.5342469043743 Batch Loss: 635.1624 + 265.1294 + 51.2065 = 951.4984, LR: 0.000017\n",
      "Epoch [11/12], Step [4916 / 16384], Running Loss: 986.9051206075542 Batch Loss: 591.4005 + 362.5736 + 52.0757 = 1006.0498, LR: 0.000013\n",
      "Epoch [11/12], Step [6555 / 16384], Running Loss: 985.3883136332653 Batch Loss: 555.4045 + 210.7962 + 47.9400 = 814.1407, LR: 0.000010\n",
      "Epoch [11/12], Step [8194 / 16384], Running Loss: 984.335983795855 Batch Loss: 660.0696 + 237.8273 + 49.1208 = 947.0177, LR: 0.000007\n",
      "Epoch [11/12], Step [9833 / 16384], Running Loss: 982.5918796703004 Batch Loss: 626.0458 + 381.2638 + 48.7826 = 1056.0922, LR: 0.000004\n",
      "Epoch [11/12], Step [11472 / 16384], Running Loss: 981.5644686644135 Batch Loss: 667.9575 + 320.6060 + 55.0197 = 1043.5833, LR: 0.000002\n",
      "Epoch [11/12], Step [13111 / 16384], Running Loss: 980.4619099801455 Batch Loss: 661.3320 + 372.9763 + 52.7492 = 1087.0576, LR: 0.000001\n",
      "Epoch [11/12], Step [14750 / 16384], Running Loss: 979.7916062563025 Batch Loss: 552.0502 + 253.0084 + 46.9588 = 852.0175, LR: 0.000000\n",
      "Epoch [11/12], Training Loss: 979.4667, Validation Loss: 896.2351, Validation Policy Acc: {'strong': tensor(0.9321, device='mps:0'), 'weak': tensor(0.9946, device='mps:0')}\n",
      "Validation loss improved. Saved best model to v1_0_best_model.pth\n",
      "Finished Training\n",
      "Best validation acc achieved: 0.9321\n"
     ]
    }
   ],
   "source": [
    "arch = \"v1_0\"\n",
    "\n",
    "checkpoint_file = f\"{arch}_checkpoint.pth\"\n",
    "best_model_file = f\"{arch}_best_model.pth\"\n",
    "\n",
    "load_file = f\"{arch}_starting_checkpoint.pth\"\n",
    "\n",
    "weights = load_weights_only(load_file)\n",
    "if weights:\n",
    "    print(\"Keys in loaded weights:\", weights.keys())\n",
    "    try:\n",
    "        model.load_state_dict(weights)\n",
    "        print(\"Successfully loaded weights into a new model.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load weights into a new model: {e}\")\n",
    "\n",
    "args = dict(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    model=model,\n",
    "    batch_size=2 ** 11,\n",
    "    learning_rate=1e-3,\n",
    "    epochs=12,\n",
    "    subsample_size=2 ** 25,\n",
    "    warmup_fraction=0.2,\n",
    "    weight_decay=0.06,\n",
    "    checkpoint_path=checkpoint_file,\n",
    "    best_model_path=best_model_file,\n",
    ")\n",
    "\n",
    "train.train_model(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61cc8b66",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2025-07-07T08:21:31.968245Z",
     "iopub.status.idle": "2025-07-07T08:21:31.968772Z",
     "shell.execute_reply": "2025-07-07T08:21:31.96862Z",
     "shell.execute_reply.started": "2025-07-07T08:21:31.968604Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os._exit(00)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7289025,
     "sourceId": 11619068,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 378556,
     "modelInstanceId": 357233,
     "sourceId": 438795,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "timm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 211.963565,
   "end_time": "2025-07-09T12:36:39.244338",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-09T12:33:07.280773",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
